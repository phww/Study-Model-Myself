2021-06-28::16-09-27
=============== args ===============
vocab_path:/home/ph/Dataset/VideoCaption/vocab.pkl
image_root_train:/home/ph/Dataset/VideoCaption/generateImgs/train
image_root_val:/home/ph/Dataset/VideoCaption/generateImgs/val
caption_path:/home/ph/Dataset/VideoCaption/info.json
epochs:150
batch_size_train:5
batch_size_val:5
encoder_init_lr:0.0001
decoder_init_lr:0.0001
att_dim:512
decoder_dim:512
embed_dim:512
continue_model:Show-Attend-And-Tell/check_point_fusion/epoch30.pth
use_fusion:True
*************** epoch:1 ***************
loss: 9.32532	cur:[0]\[1900]
loss: 2.84213	cur:[100]\[1900]
2021-06-28::16-11-01
=============== template config ===============
optimizer_list:[Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
), Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)]
criterion:CrossEntropyLoss()
log_per_step:20
global_step:0
global_step_eval:0
epoch:1
lr_scheduler_list:[<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b036c6100>, <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b036c60d0>]
lr_scheduler_type:metric
device:cuda
ckpt_dir:./check_point_fusion
=============== args ===============
vocab_path:/home/ph/Dataset/VideoCaption/vocab.pkl
image_root_train:/home/ph/Dataset/VideoCaption/generateImgs/train
image_root_val:/home/ph/Dataset/VideoCaption/generateImgs/val
caption_path:/home/ph/Dataset/VideoCaption/info.json
epochs:150
batch_size_train:5
batch_size_val:5
encoder_init_lr:0.0001
decoder_init_lr:0.0001
att_dim:512
decoder_dim:512
embed_dim:512
continue_model:Show-Attend-And-Tell/check_point_fusion/epoch30.pth
use_fusion:True
2021-06-28::16-12-08
=============== template config ===============
optimizer_list:[Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
), Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)]
criterion:CrossEntropyLoss()
log_per_step:20
global_step:0
global_step_eval:0
epoch:1
lr_scheduler_list:[<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2bd113af40>, <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2bd1153040>]
lr_scheduler_type:metric
device:cuda
ckpt_dir:./check_point_fusion
=============== args ===============
vocab_path:/home/ph/Dataset/VideoCaption/vocab.pkl
image_root_train:/home/ph/Dataset/VideoCaption/generateImgs/train
image_root_val:/home/ph/Dataset/VideoCaption/generateImgs/val
caption_path:/home/ph/Dataset/VideoCaption/info.json
epochs:150
batch_size_train:5
batch_size_val:5
encoder_init_lr:1e-05
decoder_init_lr:1e-05
att_dim:512
decoder_dim:512
embed_dim:512
continue_model:/home/ph/Desktop/Tutorials/Study-Model-Myself/Show-Attend-And-Tell/check_point_fusion/epoch30.pth
use_fusion:True
load model from /home/ph/Desktop/Tutorials/Study-Model-Myself/Show-Attend-And-Tell/check_point_fusion/epoch30.pth
*************** epoch:30 ***************
loss: 1.37015	cur:[100]\[1900]
loss: 1.44359	cur:[200]\[1900]
loss: 1.42984	cur:[300]\[1900]
loss: 1.37541	cur:[400]\[1900]
loss: 1.37483	cur:[500]\[1900]
loss: 1.40566	cur:[600]\[1900]
loss: 1.43345	cur:[700]\[1900]
loss: 1.37673	cur:[800]\[1900]
loss: 1.34801	cur:[900]\[1900]
loss: 1.30917	cur:[1000]\[1900]
loss: 1.34498	cur:[1100]\[1900]
loss: 1.40615	cur:[1200]\[1900]
loss: 1.38768	cur:[1300]\[1900]
loss: 1.37725	cur:[1400]\[1900]
loss: 1.27977	cur:[1500]\[1900]
loss: 1.42947	cur:[1600]\[1900]
loss: 1.36885	cur:[1700]\[1900]
loss: 1.34448	cur:[1800]\[1900]
loss: 1.35828	cur:[1900]\[1900]
epoch:30	avg_epoch_loss:1.37704
--------------- Evaluation ---------------
loss: 1.52084	cur:[100]\[300]
loss: 1.49060	cur:[200]\[300]
loss: 1.44161	cur:[300]\[300]
save model at ./check_point_fusion/epoch30.pth
epoch:30	avg_epoch_loss:1.48435
epoch:30	bleu:0.03108
*************** epoch:31 ***************
loss: 1.31839	cur:[100]\[1900]
loss: 1.33362	cur:[200]\[1900]
loss: 1.33806	cur:[300]\[1900]
loss: 1.33013	cur:[400]\[1900]
loss: 1.37285	cur:[500]\[1900]
loss: 1.39883	cur:[600]\[1900]
loss: 1.30260	cur:[700]\[1900]
loss: 1.32307	cur:[800]\[1900]
loss: 1.33712	cur:[900]\[1900]
loss: 1.35808	cur:[1000]\[1900]
loss: 1.40350	cur:[1100]\[1900]
loss: 1.41367	cur:[1200]\[1900]
loss: 1.36895	cur:[1300]\[1900]
loss: 1.36384	cur:[1400]\[1900]
loss: 1.29725	cur:[1500]\[1900]
loss: 1.36642	cur:[1600]\[1900]
loss: 1.32622	cur:[1700]\[1900]
loss: 1.35210	cur:[1800]\[1900]
loss: 1.36297	cur:[1900]\[1900]
epoch:31	avg_epoch_loss:1.35093
--------------- Evaluation ---------------
loss: 1.55174	cur:[100]\[300]
loss: 1.56332	cur:[200]\[300]
loss: 1.51114	cur:[300]\[300]
epoch:31	avg_epoch_loss:1.54207
epoch:31	bleu:0.02806
*************** epoch:32 ***************
loss: 1.34128	cur:[100]\[1900]
loss: 1.27241	cur:[200]\[1900]
loss: 1.39517	cur:[300]\[1900]
loss: 1.33882	cur:[400]\[1900]
loss: 1.36338	cur:[500]\[1900]
loss: 1.36657	cur:[600]\[1900]
loss: 1.31854	cur:[700]\[1900]
loss: 1.35842	cur:[800]\[1900]
loss: 1.32473	cur:[900]\[1900]
loss: 1.38725	cur:[1000]\[1900]
loss: 1.34265	cur:[1100]\[1900]
loss: 1.33954	cur:[1200]\[1900]
loss: 1.36268	cur:[1300]\[1900]
loss: 1.25949	cur:[1400]\[1900]
loss: 1.24078	cur:[1500]\[1900]
loss: 1.34422	cur:[1600]\[1900]
loss: 1.26830	cur:[1700]\[1900]
loss: 1.35428	cur:[1800]\[1900]
loss: 1.36008	cur:[1900]\[1900]
epoch:32	avg_epoch_loss:1.33361
--------------- Evaluation ---------------
loss: 1.48362	cur:[100]\[300]
loss: 1.52205	cur:[200]\[300]
loss: 1.56179	cur:[300]\[300]
save model at ./check_point_fusion/best.pth
epoch:32	avg_epoch_loss:1.52249
epoch:32	bleu:0.04015
*************** epoch:33 ***************
loss: 1.31264	cur:[100]\[1900]
loss: 1.33063	cur:[200]\[1900]
loss: 1.33009	cur:[300]\[1900]
loss: 1.36772	cur:[400]\[1900]
loss: 1.23811	cur:[500]\[1900]
loss: 1.37602	cur:[600]\[1900]
loss: 1.37718	cur:[700]\[1900]
loss: 1.31833	cur:[800]\[1900]
loss: 1.34979	cur:[900]\[1900]
loss: 1.37623	cur:[1000]\[1900]
loss: 1.31363	cur:[1100]\[1900]
loss: 1.31639	cur:[1200]\[1900]
loss: 1.32139	cur:[1300]\[1900]
loss: 1.37652	cur:[1400]\[1900]
loss: 1.31800	cur:[1500]\[1900]
loss: 1.28252	cur:[1600]\[1900]
loss: 1.35546	cur:[1700]\[1900]
loss: 1.32083	cur:[1800]\[1900]
loss: 1.33913	cur:[1900]\[1900]
epoch:33	avg_epoch_loss:1.33267
--------------- Evaluation ---------------
loss: 1.57785	cur:[100]\[300]
loss: 1.57014	cur:[200]\[300]
loss: 1.44059	cur:[300]\[300]
epoch:33	avg_epoch_loss:1.52953
epoch:33	bleu:0.02963
*************** epoch:34 ***************
loss: 1.27622	cur:[100]\[1900]
loss: 1.32317	cur:[200]\[1900]
loss: 1.31655	cur:[300]\[1900]
loss: 1.36471	cur:[400]\[1900]
loss: 1.30720	cur:[500]\[1900]
loss: 1.30616	cur:[600]\[1900]
loss: 1.31205	cur:[700]\[1900]
loss: 1.40188	cur:[800]\[1900]
loss: 1.29676	cur:[900]\[1900]
loss: 1.29422	cur:[1000]\[1900]
loss: 1.24969	cur:[1100]\[1900]
loss: 1.25271	cur:[1200]\[1900]
loss: 1.28721	cur:[1300]\[1900]
loss: 1.28467	cur:[1400]\[1900]
loss: 1.34020	cur:[1500]\[1900]
loss: 1.33691	cur:[1600]\[1900]
loss: 1.40458	cur:[1700]\[1900]
loss: 1.34804	cur:[1800]\[1900]
loss: 1.29630	cur:[1900]\[1900]
epoch:34	avg_epoch_loss:1.31575
--------------- Evaluation ---------------
loss: 1.59374	cur:[100]\[300]
loss: 1.47841	cur:[200]\[300]
loss: 1.54893	cur:[300]\[300]
epoch:34	avg_epoch_loss:1.54036
epoch:34	bleu:0.03884
*************** epoch:35 ***************
loss: 1.37535	cur:[100]\[1900]
loss: 1.43899	cur:[200]\[1900]
loss: 1.31870	cur:[300]\[1900]
loss: 1.30956	cur:[400]\[1900]
loss: 1.30318	cur:[500]\[1900]
loss: 1.25943	cur:[600]\[1900]
loss: 1.24581	cur:[700]\[1900]
loss: 1.37664	cur:[800]\[1900]
loss: 1.38079	cur:[900]\[1900]
loss: 1.30561	cur:[1000]\[1900]
loss: 1.25612	cur:[1100]\[1900]
loss: 1.31768	cur:[1200]\[1900]
loss: 1.29700	cur:[1300]\[1900]
loss: 1.26639	cur:[1400]\[1900]
loss: 1.35796	cur:[1500]\[1900]
loss: 1.30505	cur:[1600]\[1900]
loss: 1.31279	cur:[1700]\[1900]
loss: 1.33307	cur:[1800]\[1900]
loss: 1.35102	cur:[1900]\[1900]
epoch:35	avg_epoch_loss:1.32164
--------------- Evaluation ---------------
loss: 1.56443	cur:[100]\[300]
loss: 1.55023	cur:[200]\[300]
loss: 1.43121	cur:[300]\[300]
epoch:35	avg_epoch_loss:1.51529
epoch:35	bleu:0.03680
*************** epoch:36 ***************
loss: 1.28427	cur:[100]\[1900]
loss: 1.38485	cur:[200]\[1900]
loss: 1.29518	cur:[300]\[1900]
loss: 1.30514	cur:[400]\[1900]
loss: 1.30038	cur:[500]\[1900]
loss: 1.25222	cur:[600]\[1900]
loss: 1.35883	cur:[700]\[1900]
loss: 1.31676	cur:[800]\[1900]
loss: 1.31288	cur:[900]\[1900]
loss: 1.27307	cur:[1000]\[1900]
loss: 1.22969	cur:[1100]\[1900]
loss: 1.27596	cur:[1200]\[1900]
loss: 1.36820	cur:[1300]\[1900]
loss: 1.31954	cur:[1400]\[1900]
loss: 1.31423	cur:[1500]\[1900]
loss: 1.25943	cur:[1600]\[1900]
loss: 1.28758	cur:[1700]\[1900]
loss: 1.34293	cur:[1800]\[1900]
loss: 1.30846	cur:[1900]\[1900]
epoch:36	avg_epoch_loss:1.30471
--------------- Evaluation ---------------
loss: 1.47472	cur:[100]\[300]
loss: 1.52890	cur:[200]\[300]
loss: 1.45365	cur:[300]\[300]
epoch:36	avg_epoch_loss:1.48576
epoch:36	bleu:0.03790
*************** epoch:37 ***************
loss: 1.27296	cur:[100]\[1900]
loss: 1.26834	cur:[200]\[1900]
loss: 1.30606	cur:[300]\[1900]
loss: 1.28948	cur:[400]\[1900]
loss: 1.29632	cur:[500]\[1900]
loss: 1.24210	cur:[600]\[1900]
loss: 1.29031	cur:[700]\[1900]
loss: 1.23702	cur:[800]\[1900]
loss: 1.32143	cur:[900]\[1900]
loss: 1.21650	cur:[1000]\[1900]
loss: 1.25922	cur:[1100]\[1900]
loss: 1.24083	cur:[1200]\[1900]
loss: 1.24823	cur:[1300]\[1900]
loss: 1.28015	cur:[1400]\[1900]
loss: 1.27616	cur:[1500]\[1900]
loss: 1.29927	cur:[1600]\[1900]
loss: 1.31574	cur:[1700]\[1900]
loss: 1.27863	cur:[1800]\[1900]
loss: 1.28902	cur:[1900]\[1900]
epoch:37	avg_epoch_loss:1.27515
--------------- Evaluation ---------------
loss: 1.44340	cur:[100]\[300]
loss: 1.48945	cur:[200]\[300]
loss: 1.45440	cur:[300]\[300]
epoch:37	avg_epoch_loss:1.46242
epoch:37	bleu:0.03984
*************** epoch:38 ***************
loss: 1.30223	cur:[100]\[1900]
loss: 1.34039	cur:[200]\[1900]
loss: 1.27950	cur:[300]\[1900]
loss: 1.33637	cur:[400]\[1900]
loss: 1.29648	cur:[500]\[1900]
loss: 1.32119	cur:[600]\[1900]
loss: 1.33828	cur:[700]\[1900]
loss: 1.26140	cur:[800]\[1900]
loss: 1.27599	cur:[900]\[1900]
loss: 1.23842	cur:[1000]\[1900]
loss: 1.29934	cur:[1100]\[1900]
loss: 1.32286	cur:[1200]\[1900]
loss: 1.27477	cur:[1300]\[1900]
loss: 1.20903	cur:[1400]\[1900]
loss: 1.26928	cur:[1500]\[1900]
loss: 1.27827	cur:[1600]\[1900]
loss: 1.28779	cur:[1700]\[1900]
loss: 1.30206	cur:[1800]\[1900]
loss: 1.27761	cur:[1900]\[1900]
epoch:38	avg_epoch_loss:1.29007
--------------- Evaluation ---------------
loss: 1.57822	cur:[100]\[300]
loss: 1.46727	cur:[200]\[300]
loss: 1.54425	cur:[300]\[300]
epoch:38	avg_epoch_loss:1.52991
epoch:38	bleu:0.02241
Epoch     9: reducing learning rate of group 0 to 1.0000e-05.
Epoch     9: reducing learning rate of group 0 to 1.0000e-05.
*************** epoch:39 ***************
loss: 1.24970	cur:[100]\[1900]
loss: 1.27040	cur:[200]\[1900]
loss: 1.24473	cur:[300]\[1900]
loss: 1.31745	cur:[400]\[1900]
loss: 1.31999	cur:[500]\[1900]
loss: 1.21734	cur:[600]\[1900]
loss: 1.22922	cur:[700]\[1900]
loss: 1.33107	cur:[800]\[1900]
loss: 1.30783	cur:[900]\[1900]
loss: 1.35128	cur:[1000]\[1900]
loss: 1.22083	cur:[1100]\[1900]
loss: 1.30035	cur:[1200]\[1900]
loss: 1.26435	cur:[1300]\[1900]
loss: 1.27192	cur:[1400]\[1900]
loss: 1.29735	cur:[1500]\[1900]
loss: 1.27843	cur:[1600]\[1900]
loss: 1.25435	cur:[1700]\[1900]
loss: 1.24937	cur:[1800]\[1900]
loss: 1.29586	cur:[1900]\[1900]
epoch:39	avg_epoch_loss:1.27747
--------------- Evaluation ---------------
loss: 1.60296	cur:[100]\[300]
loss: 1.52462	cur:[200]\[300]
loss: 1.52778	cur:[300]\[300]
epoch:39	avg_epoch_loss:1.55179
epoch:39	bleu:0.03115
*************** epoch:40 ***************
loss: 1.22248	cur:[100]\[1900]
loss: 1.31809	cur:[200]\[1900]
loss: 1.27041	cur:[300]\[1900]
loss: 1.31778	cur:[400]\[1900]
loss: 1.28424	cur:[500]\[1900]
loss: 1.27146	cur:[600]\[1900]
loss: 1.29452	cur:[700]\[1900]
loss: 1.27311	cur:[800]\[1900]
loss: 1.31952	cur:[900]\[1900]
loss: 1.17850	cur:[1000]\[1900]
loss: 1.22501	cur:[1100]\[1900]
loss: 1.20859	cur:[1200]\[1900]
loss: 1.27212	cur:[1300]\[1900]
loss: 1.28897	cur:[1400]\[1900]
loss: 1.28183	cur:[1500]\[1900]
loss: 1.31023	cur:[1600]\[1900]
loss: 1.26360	cur:[1700]\[1900]
loss: 1.24454	cur:[1800]\[1900]
loss: 1.19796	cur:[1900]\[1900]
epoch:40	avg_epoch_loss:1.26542
--------------- Evaluation ---------------
loss: 1.55532	cur:[100]\[300]
loss: 1.53407	cur:[200]\[300]
loss: 1.48646	cur:[300]\[300]
save model at ./check_point_fusion/best.pth
save model at ./check_point_fusion/epoch40.pth
epoch:40	avg_epoch_loss:1.52528
epoch:40	bleu:0.04814
*************** epoch:41 ***************
loss: 1.25866	cur:[100]\[1900]
loss: 1.23364	cur:[200]\[1900]
loss: 1.18637	cur:[300]\[1900]
loss: 1.31487	cur:[400]\[1900]
loss: 1.22670	cur:[500]\[1900]
loss: 1.31449	cur:[600]\[1900]
loss: 1.26413	cur:[700]\[1900]
loss: 1.25464	cur:[800]\[1900]
loss: 1.22240	cur:[900]\[1900]
loss: 1.30340	cur:[1000]\[1900]
loss: 1.22022	cur:[1100]\[1900]
loss: 1.27939	cur:[1200]\[1900]
loss: 1.26877	cur:[1300]\[1900]
loss: 1.25932	cur:[1400]\[1900]
loss: 1.23047	cur:[1500]\[1900]
loss: 1.35002	cur:[1600]\[1900]
loss: 1.23314	cur:[1700]\[1900]
loss: 1.32278	cur:[1800]\[1900]
loss: 1.19957	cur:[1900]\[1900]
epoch:41	avg_epoch_loss:1.26016
--------------- Evaluation ---------------
loss: 1.52500	cur:[100]\[300]
loss: 1.43692	cur:[200]\[300]
loss: 1.47495	cur:[300]\[300]
epoch:41	avg_epoch_loss:1.47895
epoch:41	bleu:0.02917
*************** epoch:42 ***************
loss: 1.25481	cur:[100]\[1900]
loss: 1.28545	cur:[200]\[1900]
loss: 1.20844	cur:[300]\[1900]
loss: 1.23265	cur:[400]\[1900]
loss: 1.25747	cur:[500]\[1900]
loss: 1.22342	cur:[600]\[1900]
loss: 1.29071	cur:[700]\[1900]
loss: 1.15283	cur:[800]\[1900]
loss: 1.22855	cur:[900]\[1900]
loss: 1.29979	cur:[1000]\[1900]
loss: 1.31115	cur:[1100]\[1900]
loss: 1.24446	cur:[1200]\[1900]
loss: 1.31140	cur:[1300]\[1900]
loss: 1.23170	cur:[1400]\[1900]
loss: 1.28309	cur:[1500]\[1900]
loss: 1.24500	cur:[1600]\[1900]
loss: 1.30890	cur:[1700]\[1900]
loss: 1.18969	cur:[1800]\[1900]
loss: 1.27540	cur:[1900]\[1900]
epoch:42	avg_epoch_loss:1.25447
--------------- Evaluation ---------------
loss: 1.48196	cur:[100]\[300]
loss: 1.45744	cur:[200]\[300]
loss: 1.42471	cur:[300]\[300]
epoch:42	avg_epoch_loss:1.45470
epoch:42	bleu:0.03873
*************** epoch:43 ***************
loss: 1.23940	cur:[100]\[1900]
loss: 1.22449	cur:[200]\[1900]
loss: 1.32508	cur:[300]\[1900]
loss: 1.20562	cur:[400]\[1900]
loss: 1.26112	cur:[500]\[1900]
loss: 1.25501	cur:[600]\[1900]
loss: 1.29497	cur:[700]\[1900]
loss: 1.31552	cur:[800]\[1900]
loss: 1.26036	cur:[900]\[1900]
loss: 1.22364	cur:[1000]\[1900]
loss: 1.22129	cur:[1100]\[1900]
loss: 1.29984	cur:[1200]\[1900]
loss: 1.23948	cur:[1300]\[1900]
loss: 1.29394	cur:[1400]\[1900]
loss: 1.24689	cur:[1500]\[1900]
loss: 1.22413	cur:[1600]\[1900]
loss: 1.21811	cur:[1700]\[1900]
loss: 1.22362	cur:[1800]\[1900]
loss: 1.27623	cur:[1900]\[1900]
epoch:43	avg_epoch_loss:1.25520
--------------- Evaluation ---------------
loss: 1.58914	cur:[100]\[300]
loss: 1.44678	cur:[200]\[300]
loss: 1.43753	cur:[300]\[300]
save model at ./check_point_fusion/best.pth
epoch:43	avg_epoch_loss:1.49115
epoch:43	bleu:0.05184
*************** epoch:44 ***************
loss: 1.23945	cur:[100]\[1900]
loss: 1.22363	cur:[200]\[1900]
loss: 1.20501	cur:[300]\[1900]
loss: 1.26150	cur:[400]\[1900]
loss: 1.23096	cur:[500]\[1900]
loss: 1.25632	cur:[600]\[1900]
loss: 1.30659	cur:[700]\[1900]
loss: 1.26563	cur:[800]\[1900]
loss: 1.20079	cur:[900]\[1900]
loss: 1.23193	cur:[1000]\[1900]
loss: 1.30244	cur:[1100]\[1900]
loss: 1.26877	cur:[1200]\[1900]
loss: 1.24306	cur:[1300]\[1900]
loss: 1.25266	cur:[1400]\[1900]
loss: 1.25896	cur:[1500]\[1900]
loss: 1.28876	cur:[1600]\[1900]
loss: 1.26748	cur:[1700]\[1900]
loss: 1.28007	cur:[1800]\[1900]
loss: 1.24324	cur:[1900]\[1900]
epoch:44	avg_epoch_loss:1.25407
--------------- Evaluation ---------------
loss: 1.46219	cur:[100]\[300]
loss: 1.42465	cur:[200]\[300]
loss: 1.54359	cur:[300]\[300]
epoch:44	avg_epoch_loss:1.47681
epoch:44	bleu:0.03862
*************** epoch:45 ***************
loss: 1.21065	cur:[100]\[1900]
loss: 1.16482	cur:[200]\[1900]
loss: 1.29217	cur:[300]\[1900]
loss: 1.30666	cur:[400]\[1900]
loss: 1.19424	cur:[500]\[1900]
loss: 1.15462	cur:[600]\[1900]
loss: 1.25217	cur:[700]\[1900]
loss: 1.25694	cur:[800]\[1900]
loss: 1.21445	cur:[900]\[1900]
loss: 1.27154	cur:[1000]\[1900]
loss: 1.20155	cur:[1100]\[1900]
loss: 1.27764	cur:[1200]\[1900]
loss: 1.20046	cur:[1300]\[1900]
loss: 1.29709	cur:[1400]\[1900]
loss: 1.29851	cur:[1500]\[1900]
loss: 1.27297	cur:[1600]\[1900]
loss: 1.19876	cur:[1700]\[1900]
loss: 1.27109	cur:[1800]\[1900]
loss: 1.21477	cur:[1900]\[1900]
epoch:45	avg_epoch_loss:1.23953
--------------- Evaluation ---------------
loss: 1.46285	cur:[100]\[300]
loss: 1.47435	cur:[200]\[300]
loss: 1.54389	cur:[300]\[300]
epoch:45	avg_epoch_loss:1.49370
epoch:45	bleu:0.03556
*************** epoch:46 ***************
loss: 1.22488	cur:[100]\[1900]
loss: 1.22430	cur:[200]\[1900]
loss: 1.27282	cur:[300]\[1900]
loss: 1.23590	cur:[400]\[1900]
loss: 1.25750	cur:[500]\[1900]
loss: 1.27457	cur:[600]\[1900]
loss: 1.27406	cur:[700]\[1900]
loss: 1.22276	cur:[800]\[1900]
loss: 1.24800	cur:[900]\[1900]
loss: 1.25236	cur:[1000]\[1900]
loss: 1.20176	cur:[1100]\[1900]
loss: 1.24749	cur:[1200]\[1900]
loss: 1.21551	cur:[1300]\[1900]
loss: 1.23647	cur:[1400]\[1900]
loss: 1.17523	cur:[1500]\[1900]
loss: 1.24061	cur:[1600]\[1900]
loss: 1.21067	cur:[1700]\[1900]
loss: 1.20874	cur:[1800]\[1900]
loss: 1.21760	cur:[1900]\[1900]
epoch:46	avg_epoch_loss:1.23375
--------------- Evaluation ---------------
loss: 1.49140	cur:[100]\[300]
loss: 1.47347	cur:[200]\[300]
loss: 1.45797	cur:[300]\[300]
epoch:46	avg_epoch_loss:1.47428
epoch:46	bleu:0.03923
*************** epoch:47 ***************
loss: 1.23418	cur:[100]\[1900]
loss: 1.22262	cur:[200]\[1900]
loss: 1.14738	cur:[300]\[1900]
loss: 1.31245	cur:[400]\[1900]
loss: 1.22730	cur:[500]\[1900]
loss: 1.30265	cur:[600]\[1900]
loss: 1.27588	cur:[700]\[1900]
loss: 1.21417	cur:[800]\[1900]
loss: 1.26565	cur:[900]\[1900]
loss: 1.16673	cur:[1000]\[1900]
loss: 1.21993	cur:[1100]\[1900]
loss: 1.23152	cur:[1200]\[1900]
loss: 1.14283	cur:[1300]\[1900]
loss: 1.20569	cur:[1400]\[1900]
loss: 1.30195	cur:[1500]\[1900]
loss: 1.21350	cur:[1600]\[1900]
loss: 1.28499	cur:[1700]\[1900]
loss: 1.23909	cur:[1800]\[1900]
loss: 1.23892	cur:[1900]\[1900]
epoch:47	avg_epoch_loss:1.23407
--------------- Evaluation ---------------
loss: 1.58880	cur:[100]\[300]
loss: 1.44641	cur:[200]\[300]
loss: 1.49649	cur:[300]\[300]
epoch:47	avg_epoch_loss:1.51057
epoch:47	bleu:0.04207
*************** epoch:48 ***************
loss: 1.22144	cur:[100]\[1900]
loss: 1.18233	cur:[200]\[1900]
loss: 1.22637	cur:[300]\[1900]
loss: 1.27030	cur:[400]\[1900]
loss: 1.27903	cur:[500]\[1900]
loss: 1.25960	cur:[600]\[1900]
loss: 1.25718	cur:[700]\[1900]
loss: 1.17221	cur:[800]\[1900]
loss: 1.26220	cur:[900]\[1900]
loss: 1.24792	cur:[1000]\[1900]
loss: 1.17112	cur:[1100]\[1900]
loss: 1.27174	cur:[1200]\[1900]
loss: 1.28379	cur:[1300]\[1900]
loss: 1.23686	cur:[1400]\[1900]
loss: 1.17976	cur:[1500]\[1900]
loss: 1.23578	cur:[1600]\[1900]
loss: 1.17571	cur:[1700]\[1900]
loss: 1.23080	cur:[1800]\[1900]
loss: 1.25657	cur:[1900]\[1900]
epoch:48	avg_epoch_loss:1.23267
--------------- Evaluation ---------------
loss: 1.60223	cur:[100]\[300]
loss: 1.43127	cur:[200]\[300]
loss: 1.48372	cur:[300]\[300]
epoch:48	avg_epoch_loss:1.50574
epoch:48	bleu:0.03952
*************** epoch:49 ***************
loss: 1.20652	cur:[100]\[1900]
loss: 1.22884	cur:[200]\[1900]
loss: 1.21648	cur:[300]\[1900]
loss: 1.20749	cur:[400]\[1900]
loss: 1.17646	cur:[500]\[1900]
loss: 1.23873	cur:[600]\[1900]
loss: 1.28927	cur:[700]\[1900]
loss: 1.22221	cur:[800]\[1900]
loss: 1.25731	cur:[900]\[1900]
loss: 1.26425	cur:[1000]\[1900]
loss: 1.20179	cur:[1100]\[1900]
loss: 1.24932	cur:[1200]\[1900]
loss: 1.23301	cur:[1300]\[1900]
loss: 1.20268	cur:[1400]\[1900]
loss: 1.28875	cur:[1500]\[1900]
loss: 1.25487	cur:[1600]\[1900]
loss: 1.25002	cur:[1700]\[1900]
loss: 1.38186	cur:[1800]\[1900]
loss: 1.23219	cur:[1900]\[1900]
epoch:49	avg_epoch_loss:1.24221
--------------- Evaluation ---------------
loss: 1.49688	cur:[100]\[300]
loss: 1.61356	cur:[200]\[300]
loss: 1.44520	cur:[300]\[300]
epoch:49	avg_epoch_loss:1.51855
epoch:49	bleu:0.02733
Epoch    20: reducing learning rate of group 0 to 1.0000e-06.
Epoch    20: reducing learning rate of group 0 to 1.0000e-06.
*************** epoch:50 ***************
loss: 1.19022	cur:[100]\[1900]
loss: 1.28118	cur:[200]\[1900]
loss: 1.21485	cur:[300]\[1900]
loss: 1.26316	cur:[400]\[1900]
loss: 1.25190	cur:[500]\[1900]
loss: 1.19312	cur:[600]\[1900]
loss: 1.22047	cur:[700]\[1900]
loss: 1.27074	cur:[800]\[1900]
loss: 1.21307	cur:[900]\[1900]
loss: 1.22490	cur:[1000]\[1900]
loss: 1.28410	cur:[1100]\[1900]
loss: 1.22385	cur:[1200]\[1900]
loss: 1.23263	cur:[1300]\[1900]
loss: 1.24200	cur:[1400]\[1900]
loss: 1.32737	cur:[1500]\[1900]
loss: 1.23885	cur:[1600]\[1900]
loss: 1.22531	cur:[1700]\[1900]
loss: 1.25834	cur:[1800]\[1900]
loss: 1.17665	cur:[1900]\[1900]
epoch:50	avg_epoch_loss:1.23856
--------------- Evaluation ---------------
loss: 1.46874	cur:[100]\[300]
loss: 1.48977	cur:[200]\[300]
loss: 1.55850	cur:[300]\[300]
save model at ./check_point_fusion/epoch50.pth
epoch:50	avg_epoch_loss:1.50567
epoch:50	bleu:0.03249
*************** epoch:51 ***************
loss: 1.24087	cur:[100]\[1900]
loss: 1.24151	cur:[200]\[1900]
loss: 1.25476	cur:[300]\[1900]
loss: 1.24343	cur:[400]\[1900]
loss: 1.23453	cur:[500]\[1900]
loss: 1.21091	cur:[600]\[1900]
loss: 1.24925	cur:[700]\[1900]
loss: 1.23859	cur:[800]\[1900]
loss: 1.23691	cur:[900]\[1900]
loss: 1.20950	cur:[1000]\[1900]
loss: 1.19641	cur:[1100]\[1900]
loss: 1.17814	cur:[1200]\[1900]
loss: 1.24573	cur:[1300]\[1900]
loss: 1.24560	cur:[1400]\[1900]
loss: 1.26031	cur:[1500]\[1900]
loss: 1.22382	cur:[1600]\[1900]
loss: 1.26294	cur:[1700]\[1900]
loss: 1.17050	cur:[1800]\[1900]
loss: 1.19828	cur:[1900]\[1900]
epoch:51	avg_epoch_loss:1.22853
--------------- Evaluation ---------------
loss: 1.45992	cur:[100]\[300]
loss: 1.46017	cur:[200]\[300]
loss: 1.59242	cur:[300]\[300]
save model at ./check_point_fusion/best.pth
epoch:51	avg_epoch_loss:1.50417
epoch:51	bleu:0.05389
*************** epoch:52 ***************
loss: 1.24067	cur:[100]\[1900]
loss: 1.19758	cur:[200]\[1900]
loss: 1.19741	cur:[300]\[1900]
loss: 1.24638	cur:[400]\[1900]
loss: 1.20818	cur:[500]\[1900]
loss: 1.29538	cur:[600]\[1900]
loss: 1.23453	cur:[700]\[1900]
loss: 1.23213	cur:[800]\[1900]
loss: 1.23619	cur:[900]\[1900]
loss: 1.27369	cur:[1000]\[1900]
loss: 1.24202	cur:[1100]\[1900]
loss: 1.25611	cur:[1200]\[1900]
loss: 1.18024	cur:[1300]\[1900]
loss: 1.23075	cur:[1400]\[1900]
loss: 1.27409	cur:[1500]\[1900]
loss: 1.18087	cur:[1600]\[1900]
loss: 1.16511	cur:[1700]\[1900]
loss: 1.31151	cur:[1800]\[1900]
loss: 1.21949	cur:[1900]\[1900]
epoch:52	avg_epoch_loss:1.23275
--------------- Evaluation ---------------
loss: 1.45464	cur:[100]\[300]
loss: 1.36278	cur:[200]\[300]
loss: 1.64183	cur:[300]\[300]
epoch:52	avg_epoch_loss:1.48642
epoch:52	bleu:0.04387
*************** epoch:53 ***************
loss: 1.25630	cur:[100]\[1900]
loss: 1.18958	cur:[200]\[1900]
loss: 1.22950	cur:[300]\[1900]
loss: 1.25443	cur:[400]\[1900]
loss: 1.20525	cur:[500]\[1900]
loss: 1.21261	cur:[600]\[1900]
loss: 1.20213	cur:[700]\[1900]
loss: 1.23173	cur:[800]\[1900]
loss: 1.22243	cur:[900]\[1900]
loss: 1.16610	cur:[1000]\[1900]
loss: 1.22089	cur:[1100]\[1900]
loss: 1.22708	cur:[1200]\[1900]
loss: 1.22643	cur:[1300]\[1900]
loss: 1.28235	cur:[1400]\[1900]
loss: 1.22233	cur:[1500]\[1900]
loss: 1.27085	cur:[1600]\[1900]
loss: 1.27053	cur:[1700]\[1900]
loss: 1.22967	cur:[1800]\[1900]
loss: 1.25579	cur:[1900]\[1900]
epoch:53	avg_epoch_loss:1.23031
--------------- Evaluation ---------------
loss: 1.49225	cur:[100]\[300]
loss: 1.43812	cur:[200]\[300]
loss: 1.57558	cur:[300]\[300]
epoch:53	avg_epoch_loss:1.50198
epoch:53	bleu:0.03480
*************** epoch:54 ***************
loss: 1.22019	cur:[100]\[1900]
loss: 1.23629	cur:[200]\[1900]
loss: 1.16610	cur:[300]\[1900]
loss: 1.21829	cur:[400]\[1900]
loss: 1.18500	cur:[500]\[1900]
loss: 1.21215	cur:[600]\[1900]
loss: 1.26975	cur:[700]\[1900]
loss: 1.26100	cur:[800]\[1900]
loss: 1.24455	cur:[900]\[1900]
loss: 1.29016	cur:[1000]\[1900]
loss: 1.24119	cur:[1100]\[1900]
loss: 1.25980	cur:[1200]\[1900]
loss: 1.19468	cur:[1300]\[1900]
loss: 1.25028	cur:[1400]\[1900]
loss: 1.28801	cur:[1500]\[1900]
loss: 1.19294	cur:[1600]\[1900]
loss: 1.21312	cur:[1700]\[1900]
loss: 1.22925	cur:[1800]\[1900]
loss: 1.18572	cur:[1900]\[1900]
epoch:54	avg_epoch_loss:1.22939
--------------- Evaluation ---------------
loss: 1.42250	cur:[100]\[300]
loss: 1.51650	cur:[200]\[300]
loss: 1.50137	cur:[300]\[300]
epoch:54	avg_epoch_loss:1.48012
epoch:54	bleu:0.04187
*************** epoch:55 ***************
loss: 1.22189	cur:[100]\[1900]
loss: 1.21521	cur:[200]\[1900]
loss: 1.22605	cur:[300]\[1900]
loss: 1.23549	cur:[400]\[1900]
loss: 1.30248	cur:[500]\[1900]
loss: 1.28453	cur:[600]\[1900]
loss: 1.17382	cur:[700]\[1900]
loss: 1.23752	cur:[800]\[1900]
loss: 1.16657	cur:[900]\[1900]
loss: 1.23866	cur:[1000]\[1900]
loss: 1.19461	cur:[1100]\[1900]
loss: 1.21820	cur:[1200]\[1900]
loss: 1.24517	cur:[1300]\[1900]
loss: 1.19442	cur:[1400]\[1900]
loss: 1.21966	cur:[1500]\[1900]
loss: 1.24420	cur:[1600]\[1900]
loss: 1.20036	cur:[1700]\[1900]
loss: 1.24973	cur:[1800]\[1900]
loss: 1.19649	cur:[1900]\[1900]
epoch:55	avg_epoch_loss:1.22448
--------------- Evaluation ---------------
loss: 1.47813	cur:[100]\[300]
loss: 1.49065	cur:[200]\[300]
loss: 1.47016	cur:[300]\[300]
epoch:55	avg_epoch_loss:1.47965
epoch:55	bleu:0.03539
*************** epoch:56 ***************
loss: 1.24632	cur:[100]\[1900]
loss: 1.22484	cur:[200]\[1900]
loss: 1.19917	cur:[300]\[1900]
loss: 1.25258	cur:[400]\[1900]
loss: 1.16292	cur:[500]\[1900]
loss: 1.22047	cur:[600]\[1900]
loss: 1.24765	cur:[700]\[1900]
loss: 1.18549	cur:[800]\[1900]
loss: 1.26122	cur:[900]\[1900]
loss: 1.23776	cur:[1000]\[1900]
loss: 1.27550	cur:[1100]\[1900]
loss: 1.29884	cur:[1200]\[1900]
loss: 1.19108	cur:[1300]\[1900]
loss: 1.24167	cur:[1400]\[1900]
loss: 1.19647	cur:[1500]\[1900]
loss: 1.24430	cur:[1600]\[1900]
loss: 1.21377	cur:[1700]\[1900]
loss: 1.20048	cur:[1800]\[1900]
loss: 1.27783	cur:[1900]\[1900]
epoch:56	avg_epoch_loss:1.23044
--------------- Evaluation ---------------
loss: 1.46726	cur:[100]\[300]
loss: 1.51878	cur:[200]\[300]
loss: 1.52812	cur:[300]\[300]
epoch:56	avg_epoch_loss:1.50472
epoch:56	bleu:0.03027
*************** epoch:57 ***************
loss: 1.24030	cur:[100]\[1900]
loss: 1.19289	cur:[200]\[1900]
loss: 1.20019	cur:[300]\[1900]
loss: 1.21283	cur:[400]\[1900]
loss: 1.22370	cur:[500]\[1900]
loss: 1.27479	cur:[600]\[1900]
loss: 1.26339	cur:[700]\[1900]
loss: 1.23853	cur:[800]\[1900]
loss: 1.24693	cur:[900]\[1900]
loss: 1.35057	cur:[1000]\[1900]
loss: 1.23327	cur:[1100]\[1900]
loss: 1.26672	cur:[1200]\[1900]
loss: 1.21175	cur:[1300]\[1900]
loss: 1.20159	cur:[1400]\[1900]
loss: 1.24656	cur:[1500]\[1900]
loss: 1.19452	cur:[1600]\[1900]
loss: 1.22419	cur:[1700]\[1900]
loss: 1.26076	cur:[1800]\[1900]
loss: 1.16925	cur:[1900]\[1900]
epoch:57	avg_epoch_loss:1.23436
--------------- Evaluation ---------------
loss: 1.46002	cur:[100]\[300]
loss: 1.48360	cur:[200]\[300]
loss: 1.48616	cur:[300]\[300]
epoch:57	avg_epoch_loss:1.47659
epoch:57	bleu:0.04580
Epoch    28: reducing learning rate of group 0 to 1.0000e-07.
Epoch    28: reducing learning rate of group 0 to 1.0000e-07.
*************** epoch:58 ***************
loss: 1.22836	cur:[100]\[1900]
loss: 1.27889	cur:[200]\[1900]
loss: 1.26371	cur:[300]\[1900]
loss: 1.26536	cur:[400]\[1900]
loss: 1.27014	cur:[500]\[1900]
loss: 1.22162	cur:[600]\[1900]
loss: 1.20080	cur:[700]\[1900]
loss: 1.23517	cur:[800]\[1900]
loss: 1.22462	cur:[900]\[1900]
loss: 1.17015	cur:[1000]\[1900]
loss: 1.27078	cur:[1100]\[1900]
loss: 1.21128	cur:[1200]\[1900]
loss: 1.17177	cur:[1300]\[1900]
loss: 1.22723	cur:[1400]\[1900]
loss: 1.20400	cur:[1500]\[1900]
loss: 1.24967	cur:[1600]\[1900]
loss: 1.25115	cur:[1700]\[1900]
loss: 1.28696	cur:[1800]\[1900]
loss: 1.25309	cur:[1900]\[1900]
epoch:58	avg_epoch_loss:1.23604
--------------- Evaluation ---------------
loss: 1.56203	cur:[100]\[300]
loss: 1.61254	cur:[200]\[300]
loss: 1.52053	cur:[300]\[300]
epoch:58	avg_epoch_loss:1.56503
epoch:58	bleu:0.03878
*************** epoch:59 ***************
loss: 1.27153	cur:[100]\[1900]
loss: 1.15775	cur:[200]\[1900]
loss: 1.24214	cur:[300]\[1900]
loss: 1.23496	cur:[400]\[1900]
loss: 1.21511	cur:[500]\[1900]
loss: 1.22870	cur:[600]\[1900]
loss: 1.21704	cur:[700]\[1900]
loss: 1.19998	cur:[800]\[1900]
loss: 1.24390	cur:[900]\[1900]
loss: 1.28589	cur:[1000]\[1900]
loss: 1.22179	cur:[1100]\[1900]
loss: 1.23527	cur:[1200]\[1900]
loss: 1.24585	cur:[1300]\[1900]
loss: 1.20343	cur:[1400]\[1900]
loss: 1.24146	cur:[1500]\[1900]
loss: 1.24919	cur:[1600]\[1900]
loss: 1.25984	cur:[1700]\[1900]
loss: 1.23149	cur:[1800]\[1900]
loss: 1.28227	cur:[1900]\[1900]
epoch:59	avg_epoch_loss:1.23514
--------------- Evaluation ---------------
loss: 1.44077	cur:[100]\[300]
loss: 1.49399	cur:[200]\[300]
loss: 1.59792	cur:[300]\[300]
epoch:59	avg_epoch_loss:1.51089
epoch:59	bleu:0.04280
*************** epoch:60 ***************
loss: 1.21039	cur:[100]\[1900]
loss: 1.16799	cur:[200]\[1900]
loss: 1.19952	cur:[300]\[1900]
loss: 1.21772	cur:[400]\[1900]
loss: 1.21872	cur:[500]\[1900]
loss: 1.22391	cur:[600]\[1900]
loss: 1.20812	cur:[700]\[1900]
loss: 1.13320	cur:[800]\[1900]
loss: 1.18323	cur:[900]\[1900]
loss: 1.21375	cur:[1000]\[1900]
loss: 1.18326	cur:[1100]\[1900]
loss: 1.26839	cur:[1200]\[1900]
loss: 1.27754	cur:[1300]\[1900]
loss: 1.23266	cur:[1400]\[1900]
loss: 1.18248	cur:[1500]\[1900]
loss: 1.19555	cur:[1600]\[1900]
loss: 1.27226	cur:[1700]\[1900]
loss: 1.25125	cur:[1800]\[1900]
loss: 1.23651	cur:[1900]\[1900]
epoch:60	avg_epoch_loss:1.21455
--------------- Evaluation ---------------
loss: 1.57143	cur:[100]\[300]
loss: 1.50284	cur:[200]\[300]
loss: 1.49927	cur:[300]\[300]
save model at ./check_point_fusion/epoch60.pth
epoch:60	avg_epoch_loss:1.52451
epoch:60	bleu:0.04034
*************** epoch:61 ***************
loss: 1.23450	cur:[100]\[1900]
loss: 1.20163	cur:[200]\[1900]
loss: 1.20283	cur:[300]\[1900]
loss: 1.21686	cur:[400]\[1900]
loss: 1.23343	cur:[500]\[1900]
loss: 1.31197	cur:[600]\[1900]
loss: 1.24588	cur:[700]\[1900]
loss: 1.24780	cur:[800]\[1900]
loss: 1.24103	cur:[900]\[1900]
loss: 1.28112	cur:[1000]\[1900]
loss: 1.26092	cur:[1100]\[1900]
loss: 1.22542	cur:[1200]\[1900]
loss: 1.24109	cur:[1300]\[1900]
loss: 1.16894	cur:[1400]\[1900]
loss: 1.19819	cur:[1500]\[1900]
loss: 1.24069	cur:[1600]\[1900]
loss: 1.20792	cur:[1700]\[1900]
loss: 1.21392	cur:[1800]\[1900]
loss: 1.10098	cur:[1900]\[1900]
epoch:61	avg_epoch_loss:1.22501
--------------- Evaluation ---------------
loss: 1.41509	cur:[100]\[300]
loss: 1.55236	cur:[200]\[300]
loss: 1.49954	cur:[300]\[300]
epoch:61	avg_epoch_loss:1.48900
epoch:61	bleu:0.03839
*************** epoch:62 ***************
loss: 1.20998	cur:[100]\[1900]
loss: 1.28549	cur:[200]\[1900]
loss: 1.21128	cur:[300]\[1900]
loss: 1.28998	cur:[400]\[1900]
loss: 1.22236	cur:[500]\[1900]
loss: 1.20428	cur:[600]\[1900]
loss: 1.21274	cur:[700]\[1900]
loss: 1.25662	cur:[800]\[1900]
loss: 1.25023	cur:[900]\[1900]
loss: 1.26001	cur:[1000]\[1900]
loss: 1.27980	cur:[1100]\[1900]
loss: 1.31860	cur:[1200]\[1900]
loss: 1.20221	cur:[1300]\[1900]
loss: 1.22534	cur:[1400]\[1900]
loss: 1.28169	cur:[1500]\[1900]
loss: 1.16744	cur:[1600]\[1900]
loss: 1.26017	cur:[1700]\[1900]
loss: 1.20490	cur:[1800]\[1900]
loss: 1.21050	cur:[1900]\[1900]
epoch:62	avg_epoch_loss:1.23967
--------------- Evaluation ---------------
loss: 1.48211	cur:[100]\[300]
loss: 1.48433	cur:[200]\[300]
loss: 1.48418	cur:[300]\[300]
epoch:62	avg_epoch_loss:1.48354
epoch:62	bleu:0.02677
*************** epoch:63 ***************
loss: 1.18999	cur:[100]\[1900]
loss: 1.20126	cur:[200]\[1900]
loss: 1.21469	cur:[300]\[1900]
loss: 1.19367	cur:[400]\[1900]
loss: 1.21943	cur:[500]\[1900]
loss: 1.19942	cur:[600]\[1900]
loss: 1.25106	cur:[700]\[1900]
loss: 1.22979	cur:[800]\[1900]
loss: 1.26733	cur:[900]\[1900]
loss: 1.28080	cur:[1000]\[1900]
loss: 1.25780	cur:[1100]\[1900]
loss: 1.21281	cur:[1200]\[1900]
loss: 1.15521	cur:[1300]\[1900]
loss: 1.22967	cur:[1400]\[1900]
loss: 1.17788	cur:[1500]\[1900]
loss: 1.26150	cur:[1600]\[1900]
loss: 1.23929	cur:[1700]\[1900]
loss: 1.21492	cur:[1800]\[1900]
loss: 1.16746	cur:[1900]\[1900]
epoch:63	avg_epoch_loss:1.21916
--------------- Evaluation ---------------
loss: 1.47378	cur:[100]\[300]
loss: 1.51440	cur:[200]\[300]
loss: 1.48368	cur:[300]\[300]
epoch:63	avg_epoch_loss:1.49062
epoch:63	bleu:0.03519
*************** epoch:64 ***************
loss: 1.28254	cur:[100]\[1900]
loss: 1.21706	cur:[200]\[1900]
loss: 1.25507	cur:[300]\[1900]
loss: 1.22036	cur:[400]\[1900]
loss: 1.26502	cur:[500]\[1900]
loss: 1.21177	cur:[600]\[1900]
loss: 1.27528	cur:[700]\[1900]
loss: 1.20627	cur:[800]\[1900]
loss: 1.20392	cur:[900]\[1900]
loss: 1.18201	cur:[1000]\[1900]
loss: 1.18709	cur:[1100]\[1900]
loss: 1.22659	cur:[1200]\[1900]
loss: 1.19697	cur:[1300]\[1900]
loss: 1.30025	cur:[1400]\[1900]
loss: 1.19528	cur:[1500]\[1900]
loss: 1.25273	cur:[1600]\[1900]
loss: 1.24036	cur:[1700]\[1900]
loss: 1.23814	cur:[1800]\[1900]
loss: 1.21456	cur:[1900]\[1900]
epoch:64	avg_epoch_loss:1.23007
--------------- Evaluation ---------------
loss: 1.57015	cur:[100]\[300]
loss: 1.41195	cur:[200]\[300]
loss: 1.45023	cur:[300]\[300]
epoch:64	avg_epoch_loss:1.47744
epoch:64	bleu:0.03600
*************** epoch:65 ***************
2021-06-28::18-57-51
=============== template config ===============
optimizer_list:[Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
), Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)]
criterion:CrossEntropyLoss()
log_per_step:20
global_step:0
global_step_eval:0
epoch:1
lr_scheduler_list:[<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61bee95fa0>, <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61bee95f10>]
lr_scheduler_type:loss
device:cuda
ckpt_dir:./check_point_fusion
=============== args ===============
vocab_path:/home/ph/Dataset/VideoCaption/vocab.pkl
image_root_train:/home/ph/Dataset/VideoCaption/generateImgs/train
image_root_val:/home/ph/Dataset/VideoCaption/generateImgs/val
caption_path:/home/ph/Dataset/VideoCaption/info.json
epochs:200
batch_size_train:5
batch_size_val:5
encoder_init_lr:1e-05
decoder_init_lr:1e-05
att_dim:512
decoder_dim:512
embed_dim:512
continue_model:None
use_fusion:True
*************** epoch:1 ***************
loss: 9.47408	cur:[0]\[1900]
loss: 3.09402	cur:[100]\[1900]
2021-06-28::18-59-45
=============== template config ===============
optimizer_list:[Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
), Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)]
criterion:CrossEntropyLoss()
log_per_step:20
global_step:0
global_step_eval:0
epoch:1
lr_scheduler_list:[<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14db7bef40>, <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14db7bef10>]
lr_scheduler_type:loss
device:cuda
ckpt_dir:./check_point_fusion
=============== args ===============
vocab_path:/home/ph/Dataset/VideoCaption/vocab.pkl
image_root_train:/home/ph/Dataset/VideoCaption/generateImgs/train
image_root_val:/home/ph/Dataset/VideoCaption/generateImgs/val
caption_path:/home/ph/Dataset/VideoCaption/info.json
epochs:200
batch_size_train:5
batch_size_val:5
encoder_init_lr:1e-05
decoder_init_lr:1e-05
att_dim:512
decoder_dim:512
embed_dim:512
continue_model:/home/ph/Desktop/Tutorials/Study-Model-Myself/Show-Attend-And-Tell/check_point_fusion/epoch50.pth
use_fusion:True
load model from /home/ph/Desktop/Tutorials/Study-Model-Myself/Show-Attend-And-Tell/check_point_fusion/epoch50.pth
*************** epoch:50 ***************
loss: 1.27344	cur:[100]\[1900]
loss: 1.24664	cur:[200]\[1900]
loss: 1.30361	cur:[300]\[1900]
loss: 1.25625	cur:[400]\[1900]
loss: 1.22469	cur:[500]\[1900]
loss: 1.21494	cur:[600]\[1900]
loss: 1.20531	cur:[700]\[1900]
loss: 1.21757	cur:[800]\[1900]
loss: 1.25929	cur:[900]\[1900]
loss: 1.23778	cur:[1000]\[1900]
loss: 1.24389	cur:[1100]\[1900]
loss: 1.19861	cur:[1200]\[1900]
loss: 1.20179	cur:[1300]\[1900]
loss: 1.24874	cur:[1400]\[1900]
loss: 1.21432	cur:[1500]\[1900]
loss: 1.24315	cur:[1600]\[1900]
loss: 1.33054	cur:[1700]\[1900]
loss: 1.25037	cur:[1800]\[1900]
loss: 1.24071	cur:[1900]\[1900]
epoch:50	avg_epoch_loss:1.24272
--------------- Evaluation ---------------
loss: 1.45859	cur:[100]\[300]
loss: 1.62699	cur:[200]\[300]
loss: 1.61902	cur:[300]\[300]
save model at ./check_point_fusion/epoch50.pth
epoch:50	avg_epoch_loss:1.56820
epoch:50	bleu:0.03083
*************** epoch:51 ***************
loss: 1.26235	cur:[100]\[1900]
loss: 1.25965	cur:[200]\[1900]
loss: 1.28297	cur:[300]\[1900]
loss: 1.19993	cur:[400]\[1900]
loss: 1.23810	cur:[500]\[1900]
loss: 1.22852	cur:[600]\[1900]
loss: 1.20950	cur:[700]\[1900]
loss: 1.20248	cur:[800]\[1900]
loss: 1.23779	cur:[900]\[1900]
loss: 1.20189	cur:[1000]\[1900]
loss: 1.23703	cur:[1100]\[1900]
loss: 1.26811	cur:[1200]\[1900]
loss: 1.23803	cur:[1300]\[1900]
loss: 1.31105	cur:[1400]\[1900]
loss: 1.30836	cur:[1500]\[1900]
loss: 1.20509	cur:[1600]\[1900]
loss: 1.23615	cur:[1700]\[1900]
loss: 1.20628	cur:[1800]\[1900]
loss: 1.28367	cur:[1900]\[1900]
epoch:51	avg_epoch_loss:1.24300
--------------- Evaluation ---------------
loss: 1.43239	cur:[100]\[300]
loss: 1.52979	cur:[200]\[300]
loss: 1.50348	cur:[300]\[300]
epoch:51	avg_epoch_loss:1.48855
epoch:51	bleu:0.01994
*************** epoch:52 ***************
loss: 1.27757	cur:[100]\[1900]
loss: 1.20979	cur:[200]\[1900]
loss: 1.30987	cur:[300]\[1900]
loss: 1.16246	cur:[400]\[1900]
loss: 1.24454	cur:[500]\[1900]
loss: 1.20599	cur:[600]\[1900]
loss: 1.19405	cur:[700]\[1900]
loss: 1.11840	cur:[800]\[1900]
loss: 1.24685	cur:[900]\[1900]
loss: 1.14848	cur:[1000]\[1900]
loss: 1.22979	cur:[1100]\[1900]
loss: 1.21267	cur:[1200]\[1900]
loss: 1.23780	cur:[1300]\[1900]
loss: 1.21905	cur:[1400]\[1900]
loss: 1.18719	cur:[1500]\[1900]
loss: 1.20978	cur:[1600]\[1900]
loss: 1.25186	cur:[1700]\[1900]
loss: 1.25818	cur:[1800]\[1900]
loss: 1.21686	cur:[1900]\[1900]
epoch:52	avg_epoch_loss:1.21796
--------------- Evaluation ---------------
loss: 1.51049	cur:[100]\[300]
loss: 1.52237	cur:[200]\[300]
loss: 1.52210	cur:[300]\[300]
epoch:52	avg_epoch_loss:1.51832
epoch:52	bleu:0.03343
*************** epoch:53 ***************
loss: 1.17824	cur:[100]\[1900]
loss: 1.23456	cur:[200]\[1900]
loss: 1.19528	cur:[300]\[1900]
loss: 1.26090	cur:[400]\[1900]
loss: 1.26329	cur:[500]\[1900]
loss: 1.18156	cur:[600]\[1900]
loss: 1.21181	cur:[700]\[1900]
loss: 1.23491	cur:[800]\[1900]
loss: 1.20363	cur:[900]\[1900]
loss: 1.22635	cur:[1000]\[1900]
loss: 1.22333	cur:[1100]\[1900]
loss: 1.20629	cur:[1200]\[1900]
loss: 1.20469	cur:[1300]\[1900]
loss: 1.29315	cur:[1400]\[1900]
loss: 1.15963	cur:[1500]\[1900]
loss: 1.18734	cur:[1600]\[1900]
loss: 1.26269	cur:[1700]\[1900]
loss: 1.21047	cur:[1800]\[1900]
loss: 1.20685	cur:[1900]\[1900]
epoch:53	avg_epoch_loss:1.21816
--------------- Evaluation ---------------
loss: 1.47740	cur:[100]\[300]
loss: 1.50195	cur:[200]\[300]
loss: 1.58083	cur:[300]\[300]
epoch:53	avg_epoch_loss:1.52006
epoch:53	bleu:0.03889
*************** epoch:54 ***************
loss: 1.18256	cur:[100]\[1900]
loss: 1.26717	cur:[200]\[1900]
loss: 1.24573	cur:[300]\[1900]
loss: 1.27073	cur:[400]\[1900]
loss: 1.23677	cur:[500]\[1900]
loss: 1.26642	cur:[600]\[1900]
loss: 1.17941	cur:[700]\[1900]
loss: 1.20535	cur:[800]\[1900]
loss: 1.20228	cur:[900]\[1900]
loss: 1.25928	cur:[1000]\[1900]
loss: 1.22955	cur:[1100]\[1900]
loss: 1.27487	cur:[1200]\[1900]
loss: 1.24989	cur:[1300]\[1900]
loss: 1.15421	cur:[1400]\[1900]
loss: 1.20990	cur:[1500]\[1900]
loss: 1.18636	cur:[1600]\[1900]
loss: 1.16108	cur:[1700]\[1900]
loss: 1.24042	cur:[1800]\[1900]
loss: 1.24582	cur:[1900]\[1900]
epoch:54	avg_epoch_loss:1.22462
--------------- Evaluation ---------------
loss: 1.51838	cur:[100]\[300]
loss: 1.47586	cur:[200]\[300]
loss: 1.43126	cur:[300]\[300]
epoch:54	avg_epoch_loss:1.47516
epoch:54	bleu:0.03996
*************** epoch:55 ***************
loss: 1.22737	cur:[100]\[1900]
loss: 1.21726	cur:[200]\[1900]
loss: 1.14561	cur:[300]\[1900]
loss: 1.19420	cur:[400]\[1900]
loss: 1.23714	cur:[500]\[1900]
loss: 1.20072	cur:[600]\[1900]
loss: 1.24256	cur:[700]\[1900]
loss: 1.24341	cur:[800]\[1900]
loss: 1.18674	cur:[900]\[1900]
loss: 1.21445	cur:[1000]\[1900]
loss: 1.26412	cur:[1100]\[1900]
loss: 1.18809	cur:[1200]\[1900]
loss: 1.17002	cur:[1300]\[1900]
loss: 1.20947	cur:[1400]\[1900]
loss: 1.30313	cur:[1500]\[1900]
loss: 1.22679	cur:[1600]\[1900]
loss: 1.24369	cur:[1700]\[1900]
loss: 1.18635	cur:[1800]\[1900]
loss: 1.17399	cur:[1900]\[1900]
epoch:55	avg_epoch_loss:1.21448
--------------- Evaluation ---------------
loss: 1.55058	cur:[100]\[300]
loss: 1.49544	cur:[200]\[300]
loss: 1.61653	cur:[300]\[300]
epoch:55	avg_epoch_loss:1.55418
epoch:55	bleu:0.02701
*************** epoch:56 ***************
loss: 1.23054	cur:[100]\[1900]
loss: 1.21395	cur:[200]\[1900]
loss: 1.28581	cur:[300]\[1900]
loss: 1.24651	cur:[400]\[1900]
loss: 1.23249	cur:[500]\[1900]
loss: 1.30298	cur:[600]\[1900]
loss: 1.16623	cur:[700]\[1900]
loss: 1.26287	cur:[800]\[1900]
loss: 1.14535	cur:[900]\[1900]
loss: 1.24131	cur:[1000]\[1900]
loss: 1.23929	cur:[1100]\[1900]
loss: 1.25286	cur:[1200]\[1900]
loss: 1.19267	cur:[1300]\[1900]
loss: 1.21907	cur:[1400]\[1900]
loss: 1.22849	cur:[1500]\[1900]
loss: 1.20645	cur:[1600]\[1900]
loss: 1.18687	cur:[1700]\[1900]
loss: 1.28023	cur:[1800]\[1900]
loss: 1.24907	cur:[1900]\[1900]
epoch:56	avg_epoch_loss:1.23069
--------------- Evaluation ---------------
loss: 1.40905	cur:[100]\[300]
loss: 1.62226	cur:[200]\[300]
loss: 1.52052	cur:[300]\[300]
epoch:56	avg_epoch_loss:1.51728
epoch:56	bleu:0.03925
*************** epoch:57 ***************
loss: 1.25184	cur:[100]\[1900]
loss: 1.20824	cur:[200]\[1900]
loss: 1.26160	cur:[300]\[1900]
loss: 1.18823	cur:[400]\[1900]
loss: 1.18747	cur:[500]\[1900]
loss: 1.23203	cur:[600]\[1900]
loss: 1.16815	cur:[700]\[1900]
loss: 1.17479	cur:[800]\[1900]
loss: 1.18667	cur:[900]\[1900]
loss: 1.25904	cur:[1000]\[1900]
loss: 1.22472	cur:[1100]\[1900]
loss: 1.21277	cur:[1200]\[1900]
loss: 1.24093	cur:[1300]\[1900]
loss: 1.19321	cur:[1400]\[1900]
loss: 1.18086	cur:[1500]\[1900]
loss: 1.17441	cur:[1600]\[1900]
loss: 1.17870	cur:[1700]\[1900]
loss: 1.24797	cur:[1800]\[1900]
loss: 1.26459	cur:[1900]\[1900]
epoch:57	avg_epoch_loss:1.21243
--------------- Evaluation ---------------
loss: 1.46900	cur:[100]\[300]
loss: 1.50954	cur:[200]\[300]
loss: 1.54863	cur:[300]\[300]
epoch:57	avg_epoch_loss:1.50906
epoch:57	bleu:0.02925
*************** epoch:58 ***************
loss: 1.30763	cur:[100]\[1900]
loss: 1.21269	cur:[200]\[1900]
loss: 1.18878	cur:[300]\[1900]
loss: 1.17928	cur:[400]\[1900]
loss: 1.27324	cur:[500]\[1900]
loss: 1.16999	cur:[600]\[1900]
loss: 1.21917	cur:[700]\[1900]
loss: 1.16264	cur:[800]\[1900]
loss: 1.26022	cur:[900]\[1900]
loss: 1.17820	cur:[1000]\[1900]
loss: 1.18908	cur:[1100]\[1900]
loss: 1.22512	cur:[1200]\[1900]
loss: 1.24708	cur:[1300]\[1900]
loss: 1.25529	cur:[1400]\[1900]
loss: 1.23143	cur:[1500]\[1900]
loss: 1.22909	cur:[1600]\[1900]
loss: 1.19378	cur:[1700]\[1900]
loss: 1.17755	cur:[1800]\[1900]
loss: 1.20323	cur:[1900]\[1900]
epoch:58	avg_epoch_loss:1.21597
--------------- Evaluation ---------------
loss: 1.50126	cur:[100]\[300]
loss: 1.46213	cur:[200]\[300]
loss: 1.48094	cur:[300]\[300]
epoch:58	avg_epoch_loss:1.48144
epoch:58	bleu:0.05151
*************** epoch:59 ***************
loss: 1.21610	cur:[100]\[1900]
loss: 1.21692	cur:[200]\[1900]
loss: 1.23642	cur:[300]\[1900]
loss: 1.12998	cur:[400]\[1900]
loss: 1.21849	cur:[500]\[1900]
loss: 1.17947	cur:[600]\[1900]
loss: 1.26814	cur:[700]\[1900]
loss: 1.18264	cur:[800]\[1900]
loss: 1.16799	cur:[900]\[1900]
loss: 1.17069	cur:[1000]\[1900]
loss: 1.20416	cur:[1100]\[1900]
loss: 1.23262	cur:[1200]\[1900]
loss: 1.24341	cur:[1300]\[1900]
loss: 1.24964	cur:[1400]\[1900]
loss: 1.21382	cur:[1500]\[1900]
loss: 1.17688	cur:[1600]\[1900]
loss: 1.15123	cur:[1700]\[1900]
loss: 1.24879	cur:[1800]\[1900]
loss: 1.16787	cur:[1900]\[1900]
epoch:59	avg_epoch_loss:1.20396
--------------- Evaluation ---------------
loss: 1.64394	cur:[100]\[300]
loss: 1.54757	cur:[200]\[300]
loss: 1.43033	cur:[300]\[300]
epoch:59	avg_epoch_loss:1.54061
epoch:59	bleu:0.03140
*************** epoch:60 ***************
loss: 1.21048	cur:[100]\[1900]
loss: 1.24152	cur:[200]\[1900]
loss: 1.22735	cur:[300]\[1900]
loss: 1.27323	cur:[400]\[1900]
loss: 1.22227	cur:[500]\[1900]
loss: 1.22287	cur:[600]\[1900]
loss: 1.17004	cur:[700]\[1900]
loss: 1.21308	cur:[800]\[1900]
loss: 1.15229	cur:[900]\[1900]
loss: 1.20382	cur:[1000]\[1900]
loss: 1.26020	cur:[1100]\[1900]
loss: 1.22338	cur:[1200]\[1900]
loss: 1.20510	cur:[1300]\[1900]
loss: 1.21178	cur:[1400]\[1900]
loss: 1.22729	cur:[1500]\[1900]
loss: 1.27376	cur:[1600]\[1900]
loss: 1.17285	cur:[1700]\[1900]
loss: 1.21003	cur:[1800]\[1900]
loss: 1.18988	cur:[1900]\[1900]
epoch:60	avg_epoch_loss:1.21638
--------------- Evaluation ---------------
loss: 1.62127	cur:[100]\[300]
loss: 1.59008	cur:[200]\[300]
loss: 1.51961	cur:[300]\[300]
save model at ./check_point_fusion/epoch60.pth
epoch:60	avg_epoch_loss:1.57699
epoch:60	bleu:0.03679
Epoch    11: reducing learning rate of group 0 to 1.0000e-06.
Epoch    11: reducing learning rate of group 0 to 1.0000e-06.
*************** epoch:61 ***************
loss: 1.23355	cur:[100]\[1900]
loss: 1.27353	cur:[200]\[1900]
loss: 1.20027	cur:[300]\[1900]
loss: 1.20625	cur:[400]\[1900]
loss: 1.24771	cur:[500]\[1900]
loss: 1.25640	cur:[600]\[1900]
loss: 1.24020	cur:[700]\[1900]
loss: 1.20598	cur:[800]\[1900]
loss: 1.17845	cur:[900]\[1900]
loss: 1.19886	cur:[1000]\[1900]
loss: 1.20883	cur:[1100]\[1900]
loss: 1.17112	cur:[1200]\[1900]
loss: 1.18956	cur:[1300]\[1900]
loss: 1.17453	cur:[1400]\[1900]
loss: 1.18696	cur:[1500]\[1900]
loss: 1.17195	cur:[1600]\[1900]
loss: 1.23795	cur:[1700]\[1900]
loss: 1.23575	cur:[1800]\[1900]
loss: 1.25125	cur:[1900]\[1900]
epoch:61	avg_epoch_loss:1.21416
--------------- Evaluation ---------------
loss: 1.55964	cur:[100]\[300]
loss: 1.48092	cur:[200]\[300]
loss: 1.50324	cur:[300]\[300]
epoch:61	avg_epoch_loss:1.51460
epoch:61	bleu:0.04693
*************** epoch:62 ***************
loss: 1.21369	cur:[100]\[1900]
loss: 1.21633	cur:[200]\[1900]
loss: 1.19116	cur:[300]\[1900]
loss: 1.16807	cur:[400]\[1900]
loss: 1.26266	cur:[500]\[1900]
loss: 1.20997	cur:[600]\[1900]
loss: 1.18181	cur:[700]\[1900]
loss: 1.20672	cur:[800]\[1900]
loss: 1.18199	cur:[900]\[1900]
loss: 1.21568	cur:[1000]\[1900]
loss: 1.18382	cur:[1100]\[1900]
loss: 1.21930	cur:[1200]\[1900]
loss: 1.21053	cur:[1300]\[1900]
loss: 1.19195	cur:[1400]\[1900]
loss: 1.18906	cur:[1500]\[1900]
loss: 1.19151	cur:[1600]\[1900]
loss: 1.26617	cur:[1700]\[1900]
loss: 1.13544	cur:[1800]\[1900]
loss: 1.20216	cur:[1900]\[1900]
epoch:62	avg_epoch_loss:1.20200
--------------- Evaluation ---------------
loss: 1.41965	cur:[100]\[300]
loss: 1.45500	cur:[200]\[300]
loss: 1.52721	cur:[300]\[300]
epoch:62	avg_epoch_loss:1.46729
epoch:62	bleu:0.03002
*************** epoch:63 ***************
loss: 1.12110	cur:[100]\[1900]
loss: 1.25938	cur:[200]\[1900]
loss: 1.22403	cur:[300]\[1900]
loss: 1.21231	cur:[400]\[1900]
loss: 1.22513	cur:[500]\[1900]
loss: 1.22480	cur:[600]\[1900]
loss: 1.23044	cur:[700]\[1900]
loss: 1.21168	cur:[800]\[1900]
loss: 1.17667	cur:[900]\[1900]
loss: 1.23592	cur:[1000]\[1900]
loss: 1.16368	cur:[1100]\[1900]
loss: 1.26295	cur:[1200]\[1900]
loss: 1.19494	cur:[1300]\[1900]
loss: 1.18391	cur:[1400]\[1900]
loss: 1.16721	cur:[1500]\[1900]
loss: 1.21797	cur:[1600]\[1900]
loss: 1.14410	cur:[1700]\[1900]
loss: 1.11948	cur:[1800]\[1900]
loss: 1.22122	cur:[1900]\[1900]
epoch:63	avg_epoch_loss:1.19984
--------------- Evaluation ---------------
loss: 1.52168	cur:[100]\[300]
loss: 1.58320	cur:[200]\[300]
loss: 1.41434	cur:[300]\[300]
epoch:63	avg_epoch_loss:1.50640
epoch:63	bleu:0.04510
*************** epoch:64 ***************
loss: 1.27335	cur:[100]\[1900]
loss: 1.20074	cur:[200]\[1900]
loss: 1.19221	cur:[300]\[1900]
loss: 1.27483	cur:[400]\[1900]
loss: 1.19265	cur:[500]\[1900]
loss: 1.23487	cur:[600]\[1900]
loss: 1.22111	cur:[700]\[1900]
loss: 1.25759	cur:[800]\[1900]
loss: 1.16510	cur:[900]\[1900]
loss: 1.20822	cur:[1000]\[1900]
loss: 1.23669	cur:[1100]\[1900]
loss: 1.17365	cur:[1200]\[1900]
loss: 1.30170	cur:[1300]\[1900]
loss: 1.27502	cur:[1400]\[1900]
loss: 1.17498	cur:[1500]\[1900]
loss: 1.28187	cur:[1600]\[1900]
loss: 1.16024	cur:[1700]\[1900]
loss: 1.20346	cur:[1800]\[1900]
loss: 1.21947	cur:[1900]\[1900]
epoch:64	avg_epoch_loss:1.22357
--------------- Evaluation ---------------
loss: 1.49252	cur:[100]\[300]
loss: 1.51783	cur:[200]\[300]
loss: 1.42350	cur:[300]\[300]
epoch:64	avg_epoch_loss:1.47795
epoch:64	bleu:0.04212
*************** epoch:65 ***************
loss: 1.18293	cur:[100]\[1900]
loss: 1.29206	cur:[200]\[1900]
loss: 1.21523	cur:[300]\[1900]
loss: 1.24473	cur:[400]\[1900]
loss: 1.17010	cur:[500]\[1900]
loss: 1.12753	cur:[600]\[1900]
loss: 1.20238	cur:[700]\[1900]
loss: 1.21439	cur:[800]\[1900]
loss: 1.20267	cur:[900]\[1900]
loss: 1.21308	cur:[1000]\[1900]
loss: 1.20415	cur:[1100]\[1900]
loss: 1.20352	cur:[1200]\[1900]
loss: 1.23635	cur:[1300]\[1900]
loss: 1.23634	cur:[1400]\[1900]
loss: 1.23419	cur:[1500]\[1900]
loss: 1.21060	cur:[1600]\[1900]
loss: 1.22951	cur:[1700]\[1900]
loss: 1.23267	cur:[1800]\[1900]
loss: 1.20585	cur:[1900]\[1900]
epoch:65	avg_epoch_loss:1.21359
--------------- Evaluation ---------------
loss: 1.51151	cur:[100]\[300]
loss: 1.52447	cur:[200]\[300]
loss: 1.48778	cur:[300]\[300]
epoch:65	avg_epoch_loss:1.50792
epoch:65	bleu:0.03752
*************** epoch:66 ***************
loss: 1.20396	cur:[100]\[1900]
loss: 1.14917	cur:[200]\[1900]
loss: 1.21744	cur:[300]\[1900]
loss: 1.18104	cur:[400]\[1900]
loss: 1.16731	cur:[500]\[1900]
loss: 1.23129	cur:[600]\[1900]
loss: 1.23145	cur:[700]\[1900]
loss: 1.24629	cur:[800]\[1900]
loss: 1.21473	cur:[900]\[1900]
loss: 1.19473	cur:[1000]\[1900]
loss: 1.33462	cur:[1100]\[1900]
loss: 1.27386	cur:[1200]\[1900]
loss: 1.22528	cur:[1300]\[1900]
loss: 1.17623	cur:[1400]\[1900]
loss: 1.17115	cur:[1500]\[1900]
loss: 1.17078	cur:[1600]\[1900]
loss: 1.14548	cur:[1700]\[1900]
loss: 1.16974	cur:[1800]\[1900]
loss: 1.19456	cur:[1900]\[1900]
epoch:66	avg_epoch_loss:1.20522
--------------- Evaluation ---------------
loss: 1.51362	cur:[100]\[300]
loss: 1.48515	cur:[200]\[300]
loss: 1.46720	cur:[300]\[300]
save model at ./check_point_fusion/best.pth
epoch:66	avg_epoch_loss:1.48866
epoch:66	bleu:0.05670
*************** epoch:67 ***************
loss: 1.19495	cur:[100]\[1900]
loss: 1.17429	cur:[200]\[1900]
loss: 1.19254	cur:[300]\[1900]
loss: 1.20787	cur:[400]\[1900]
loss: 1.30407	cur:[500]\[1900]
loss: 1.21530	cur:[600]\[1900]
loss: 1.19462	cur:[700]\[1900]
loss: 1.21946	cur:[800]\[1900]
loss: 1.26984	cur:[900]\[1900]
loss: 1.17491	cur:[1000]\[1900]
loss: 1.21348	cur:[1100]\[1900]
loss: 1.19411	cur:[1200]\[1900]
loss: 1.22772	cur:[1300]\[1900]
loss: 1.25691	cur:[1400]\[1900]
loss: 1.18455	cur:[1500]\[1900]
loss: 1.19275	cur:[1600]\[1900]
loss: 1.25949	cur:[1700]\[1900]
loss: 1.18839	cur:[1800]\[1900]
loss: 1.25810	cur:[1900]\[1900]
epoch:67	avg_epoch_loss:1.21702
--------------- Evaluation ---------------
loss: 1.45052	cur:[100]\[300]
loss: 1.46943	cur:[200]\[300]
loss: 1.44135	cur:[300]\[300]
epoch:67	avg_epoch_loss:1.45377
epoch:67	bleu:0.03760
*************** epoch:68 ***************
loss: 1.20712	cur:[100]\[1900]
loss: 1.28073	cur:[200]\[1900]
loss: 1.25014	cur:[300]\[1900]
loss: 1.14858	cur:[400]\[1900]
loss: 1.12476	cur:[500]\[1900]
loss: 1.18184	cur:[600]\[1900]
loss: 1.18902	cur:[700]\[1900]
loss: 1.15406	cur:[800]\[1900]
loss: 1.18006	cur:[900]\[1900]
loss: 1.25955	cur:[1000]\[1900]
loss: 1.24805	cur:[1100]\[1900]
loss: 1.20398	cur:[1200]\[1900]
loss: 1.23023	cur:[1300]\[1900]
loss: 1.23642	cur:[1400]\[1900]
loss: 1.16255	cur:[1500]\[1900]
loss: 1.24516	cur:[1600]\[1900]
loss: 1.21481	cur:[1700]\[1900]
loss: 1.25225	cur:[1800]\[1900]
loss: 1.19495	cur:[1900]\[1900]
epoch:68	avg_epoch_loss:1.20865
--------------- Evaluation ---------------
loss: 1.43684	cur:[100]\[300]
loss: 1.49513	cur:[200]\[300]
loss: 1.40482	cur:[300]\[300]
epoch:68	avg_epoch_loss:1.44560
epoch:68	bleu:0.04259
*************** epoch:69 ***************
loss: 1.18203	cur:[100]\[1900]
loss: 1.15035	cur:[200]\[1900]
loss: 1.26552	cur:[300]\[1900]
loss: 1.17176	cur:[400]\[1900]
loss: 1.16156	cur:[500]\[1900]
loss: 1.19914	cur:[600]\[1900]
loss: 1.18342	cur:[700]\[1900]
loss: 1.21768	cur:[800]\[1900]
loss: 1.16971	cur:[900]\[1900]
loss: 1.17611	cur:[1000]\[1900]
loss: 1.22791	cur:[1100]\[1900]
loss: 1.25478	cur:[1200]\[1900]
loss: 1.18621	cur:[1300]\[1900]
loss: 1.19845	cur:[1400]\[1900]
loss: 1.20938	cur:[1500]\[1900]
loss: 1.16375	cur:[1600]\[1900]
loss: 1.15639	cur:[1700]\[1900]
loss: 1.16637	cur:[1800]\[1900]
loss: 1.15181	cur:[1900]\[1900]
epoch:69	avg_epoch_loss:1.18907
--------------- Evaluation ---------------
loss: 1.54042	cur:[100]\[300]
loss: 1.35963	cur:[200]\[300]
loss: 1.53369	cur:[300]\[300]
epoch:69	avg_epoch_loss:1.47791
epoch:69	bleu:0.04312
*************** epoch:70 ***************
loss: 1.23942	cur:[100]\[1900]
loss: 1.21158	cur:[200]\[1900]
loss: 1.20889	cur:[300]\[1900]
loss: 1.23870	cur:[400]\[1900]
loss: 1.22813	cur:[500]\[1900]
loss: 1.19361	cur:[600]\[1900]
loss: 1.23396	cur:[700]\[1900]
loss: 1.25161	cur:[800]\[1900]
loss: 1.18285	cur:[900]\[1900]
loss: 1.21734	cur:[1000]\[1900]
loss: 1.18945	cur:[1100]\[1900]
loss: 1.17794	cur:[1200]\[1900]
loss: 1.26782	cur:[1300]\[1900]
loss: 1.19751	cur:[1400]\[1900]
loss: 1.20462	cur:[1500]\[1900]
loss: 1.25598	cur:[1600]\[1900]
loss: 1.20319	cur:[1700]\[1900]
loss: 1.18273	cur:[1800]\[1900]
loss: 1.25464	cur:[1900]\[1900]
epoch:70	avg_epoch_loss:1.21789
--------------- Evaluation ---------------
loss: 1.48112	cur:[100]\[300]
loss: 1.52976	cur:[200]\[300]
loss: 1.50465	cur:[300]\[300]
save model at ./check_point_fusion/epoch70.pth
epoch:70	avg_epoch_loss:1.50518
epoch:70	bleu:0.02318
*************** epoch:71 ***************
loss: 1.27390	cur:[100]\[1900]
loss: 1.24021	cur:[200]\[1900]
loss: 1.18023	cur:[300]\[1900]
loss: 1.20167	cur:[400]\[1900]
loss: 1.30818	cur:[500]\[1900]
loss: 1.17060	cur:[600]\[1900]
loss: 1.14074	cur:[700]\[1900]
loss: 1.26671	cur:[800]\[1900]
loss: 1.20982	cur:[900]\[1900]
loss: 1.21064	cur:[1000]\[1900]
loss: 1.18689	cur:[1100]\[1900]
loss: 1.15704	cur:[1200]\[1900]
loss: 1.25514	cur:[1300]\[1900]
loss: 1.19862	cur:[1400]\[1900]
loss: 1.21230	cur:[1500]\[1900]
loss: 1.11818	cur:[1600]\[1900]
loss: 1.24574	cur:[1700]\[1900]
loss: 1.19914	cur:[1800]\[1900]
loss: 1.30846	cur:[1900]\[1900]
epoch:71	avg_epoch_loss:1.21496
--------------- Evaluation ---------------
loss: 1.54177	cur:[100]\[300]
loss: 1.46037	cur:[200]\[300]
loss: 1.47993	cur:[300]\[300]
epoch:71	avg_epoch_loss:1.49402
epoch:71	bleu:0.03857
*************** epoch:72 ***************
loss: 1.17482	cur:[100]\[1900]
loss: 1.20370	cur:[200]\[1900]
loss: 1.19628	cur:[300]\[1900]
loss: 1.24796	cur:[400]\[1900]
loss: 1.22201	cur:[500]\[1900]
loss: 1.23575	cur:[600]\[1900]
loss: 1.20426	cur:[700]\[1900]
loss: 1.15774	cur:[800]\[1900]
loss: 1.19928	cur:[900]\[1900]
loss: 1.19787	cur:[1000]\[1900]
loss: 1.14464	cur:[1100]\[1900]
loss: 1.16751	cur:[1200]\[1900]
loss: 1.18178	cur:[1300]\[1900]
loss: 1.25112	cur:[1400]\[1900]
loss: 1.19687	cur:[1500]\[1900]
loss: 1.26769	cur:[1600]\[1900]
loss: 1.26631	cur:[1700]\[1900]
loss: 1.19443	cur:[1800]\[1900]
loss: 1.24338	cur:[1900]\[1900]
epoch:72	avg_epoch_loss:1.20807
--------------- Evaluation ---------------
loss: 1.52177	cur:[100]\[300]
loss: 1.53970	cur:[200]\[300]
loss: 1.55147	cur:[300]\[300]
epoch:72	avg_epoch_loss:1.53765
epoch:72	bleu:0.03683
*************** epoch:73 ***************
loss: 1.19296	cur:[100]\[1900]
loss: 1.12823	cur:[200]\[1900]
loss: 1.27105	cur:[300]\[1900]
loss: 1.24604	cur:[400]\[1900]
loss: 1.24550	cur:[500]\[1900]
loss: 1.21317	cur:[600]\[1900]
loss: 1.19407	cur:[700]\[1900]
loss: 1.23995	cur:[800]\[1900]
loss: 1.22546	cur:[900]\[1900]
loss: 1.22563	cur:[1000]\[1900]
loss: 1.21547	cur:[1100]\[1900]
loss: 1.19294	cur:[1200]\[1900]
loss: 1.17515	cur:[1300]\[1900]
loss: 1.17615	cur:[1400]\[1900]
loss: 1.23667	cur:[1500]\[1900]
loss: 1.16876	cur:[1600]\[1900]
loss: 1.26847	cur:[1700]\[1900]
loss: 1.23507	cur:[1800]\[1900]
loss: 1.21745	cur:[1900]\[1900]
epoch:73	avg_epoch_loss:1.21412
--------------- Evaluation ---------------
loss: 1.54884	cur:[100]\[300]
loss: 1.46864	cur:[200]\[300]
loss: 1.56601	cur:[300]\[300]
epoch:73	avg_epoch_loss:1.52783
epoch:73	bleu:0.03955
*************** epoch:74 ***************
loss: 1.23084	cur:[100]\[1900]
loss: 1.21784	cur:[200]\[1900]
loss: 1.21691	cur:[300]\[1900]
loss: 1.18554	cur:[400]\[1900]
loss: 1.28724	cur:[500]\[1900]
loss: 1.14195	cur:[600]\[1900]
loss: 1.19744	cur:[700]\[1900]
loss: 1.21692	cur:[800]\[1900]
loss: 1.22153	cur:[900]\[1900]
loss: 1.23854	cur:[1000]\[1900]
loss: 1.21907	cur:[1100]\[1900]
loss: 1.15454	cur:[1200]\[1900]
loss: 1.24979	cur:[1300]\[1900]
loss: 1.14804	cur:[1400]\[1900]
loss: 1.21046	cur:[1500]\[1900]
loss: 1.17144	cur:[1600]\[1900]
loss: 1.23523	cur:[1700]\[1900]
loss: 1.24266	cur:[1800]\[1900]
loss: 1.16616	cur:[1900]\[1900]
epoch:74	avg_epoch_loss:1.20801
--------------- Evaluation ---------------
loss: 1.51463	cur:[100]\[300]
loss: 1.53275	cur:[200]\[300]
loss: 1.42867	cur:[300]\[300]
epoch:74	avg_epoch_loss:1.49202
epoch:74	bleu:0.04046
Epoch    25: reducing learning rate of group 0 to 1.0000e-07.
Epoch    25: reducing learning rate of group 0 to 1.0000e-07.
*************** epoch:75 ***************
loss: 1.18830	cur:[100]\[1900]
loss: 1.21008	cur:[200]\[1900]
loss: 1.22757	cur:[300]\[1900]
loss: 1.21838	cur:[400]\[1900]
loss: 1.21160	cur:[500]\[1900]
loss: 1.22574	cur:[600]\[1900]
loss: 1.17162	cur:[700]\[1900]
loss: 1.19430	cur:[800]\[1900]
loss: 1.23459	cur:[900]\[1900]
loss: 1.15942	cur:[1000]\[1900]
loss: 1.22120	cur:[1100]\[1900]
loss: 1.26258	cur:[1200]\[1900]
loss: 1.19932	cur:[1300]\[1900]
loss: 1.23190	cur:[1400]\[1900]
loss: 1.19736	cur:[1500]\[1900]
loss: 1.22414	cur:[1600]\[1900]
loss: 1.22087	cur:[1700]\[1900]
loss: 1.23052	cur:[1800]\[1900]
loss: 1.20508	cur:[1900]\[1900]
epoch:75	avg_epoch_loss:1.21235
--------------- Evaluation ---------------
loss: 1.52693	cur:[100]\[300]
loss: 1.59436	cur:[200]\[300]
loss: 1.53547	cur:[300]\[300]
epoch:75	avg_epoch_loss:1.55226
epoch:75	bleu:0.02882
*************** epoch:76 ***************
loss: 1.20798	cur:[100]\[1900]
loss: 1.18241	cur:[200]\[1900]
loss: 1.28184	cur:[300]\[1900]
loss: 1.17092	cur:[400]\[1900]
loss: 1.14526	cur:[500]\[1900]
loss: 1.18616	cur:[600]\[1900]
loss: 1.27378	cur:[700]\[1900]
loss: 1.20698	cur:[800]\[1900]
loss: 1.17675	cur:[900]\[1900]
loss: 1.22722	cur:[1000]\[1900]
loss: 1.25808	cur:[1100]\[1900]
loss: 1.20404	cur:[1200]\[1900]
loss: 1.21028	cur:[1300]\[1900]
loss: 1.20436	cur:[1400]\[1900]
loss: 1.19158	cur:[1500]\[1900]
loss: 1.23936	cur:[1600]\[1900]
loss: 1.18444	cur:[1700]\[1900]
loss: 1.18582	cur:[1800]\[1900]
loss: 1.24832	cur:[1900]\[1900]
epoch:76	avg_epoch_loss:1.20977
--------------- Evaluation ---------------
loss: 1.47679	cur:[100]\[300]
loss: 1.52774	cur:[200]\[300]
loss: 1.46884	cur:[300]\[300]
epoch:76	avg_epoch_loss:1.49112
epoch:76	bleu:0.03459
*************** epoch:77 ***************
loss: 1.11964	cur:[100]\[1900]
loss: 1.13611	cur:[200]\[1900]
loss: 1.24254	cur:[300]\[1900]
loss: 1.27796	cur:[400]\[1900]
loss: 1.23343	cur:[500]\[1900]
loss: 1.20066	cur:[600]\[1900]
loss: 1.22496	cur:[700]\[1900]
loss: 1.17279	cur:[800]\[1900]
loss: 1.20535	cur:[900]\[1900]
loss: 1.25764	cur:[1000]\[1900]
loss: 1.13535	cur:[1100]\[1900]
loss: 1.21351	cur:[1200]\[1900]
loss: 1.18741	cur:[1300]\[1900]
loss: 1.20214	cur:[1400]\[1900]
loss: 1.19488	cur:[1500]\[1900]
loss: 1.21131	cur:[1600]\[1900]
loss: 1.23217	cur:[1700]\[1900]
loss: 1.16876	cur:[1800]\[1900]
loss: 1.22313	cur:[1900]\[1900]
epoch:77	avg_epoch_loss:1.20209
--------------- Evaluation ---------------
loss: 1.48533	cur:[100]\[300]
loss: 1.48816	cur:[200]\[300]
loss: 1.49387	cur:[300]\[300]
epoch:77	avg_epoch_loss:1.48912
epoch:77	bleu:0.03070
*************** epoch:78 ***************
loss: 1.19052	cur:[100]\[1900]
loss: 1.22360	cur:[200]\[1900]
loss: 1.15733	cur:[300]\[1900]
loss: 1.17089	cur:[400]\[1900]
loss: 1.20927	cur:[500]\[1900]
loss: 1.26571	cur:[600]\[1900]
loss: 1.25426	cur:[700]\[1900]
loss: 1.17008	cur:[800]\[1900]
loss: 1.20421	cur:[900]\[1900]
loss: 1.22078	cur:[1000]\[1900]
loss: 1.19237	cur:[1100]\[1900]
loss: 1.12206	cur:[1200]\[1900]
loss: 1.20498	cur:[1300]\[1900]
loss: 1.21646	cur:[1400]\[1900]
loss: 1.18310	cur:[1500]\[1900]
loss: 1.23172	cur:[1600]\[1900]
loss: 1.18369	cur:[1700]\[1900]
loss: 1.21132	cur:[1800]\[1900]
loss: 1.19761	cur:[1900]\[1900]
epoch:78	avg_epoch_loss:1.20053
--------------- Evaluation ---------------
loss: 1.52318	cur:[100]\[300]
loss: 1.50187	cur:[200]\[300]
loss: 1.45000	cur:[300]\[300]
epoch:78	avg_epoch_loss:1.49168
epoch:78	bleu:0.03708
*************** epoch:79 ***************
loss: 1.15828	cur:[100]\[1900]
loss: 1.19560	cur:[200]\[1900]
loss: 1.19801	cur:[300]\[1900]
loss: 1.20237	cur:[400]\[1900]
loss: 1.25306	cur:[500]\[1900]
loss: 1.18194	cur:[600]\[1900]
loss: 1.19727	cur:[700]\[1900]
loss: 1.23914	cur:[800]\[1900]
loss: 1.20062	cur:[900]\[1900]
loss: 1.20747	cur:[1000]\[1900]
loss: 1.18869	cur:[1100]\[1900]
loss: 1.20160	cur:[1200]\[1900]
loss: 1.21581	cur:[1300]\[1900]
loss: 1.14658	cur:[1400]\[1900]
loss: 1.13713	cur:[1500]\[1900]
loss: 1.27522	cur:[1600]\[1900]
loss: 1.23762	cur:[1700]\[1900]
loss: 1.16956	cur:[1800]\[1900]
loss: 1.15625	cur:[1900]\[1900]
epoch:79	avg_epoch_loss:1.19801
--------------- Evaluation ---------------
loss: 1.41025	cur:[100]\[300]
loss: 1.42745	cur:[200]\[300]
loss: 1.48257	cur:[300]\[300]
epoch:79	avg_epoch_loss:1.44009
epoch:79	bleu:0.04675
*************** epoch:80 ***************
loss: 1.19364	cur:[100]\[1900]
loss: 1.16716	cur:[200]\[1900]
loss: 1.23687	cur:[300]\[1900]
loss: 1.26698	cur:[400]\[1900]
loss: 1.15884	cur:[500]\[1900]
loss: 1.19658	cur:[600]\[1900]
loss: 1.19166	cur:[700]\[1900]
loss: 1.14246	cur:[800]\[1900]
loss: 1.21125	cur:[900]\[1900]
loss: 1.17415	cur:[1000]\[1900]
loss: 1.18147	cur:[1100]\[1900]
loss: 1.19009	cur:[1200]\[1900]
loss: 1.21397	cur:[1300]\[1900]
loss: 1.17116	cur:[1400]\[1900]
loss: 1.21451	cur:[1500]\[1900]
loss: 1.18035	cur:[1600]\[1900]
loss: 1.21968	cur:[1700]\[1900]
loss: 1.20233	cur:[1800]\[1900]
loss: 1.21170	cur:[1900]\[1900]
epoch:80	avg_epoch_loss:1.19605
--------------- Evaluation ---------------
loss: 1.42911	cur:[100]\[300]
loss: 1.54514	cur:[200]\[300]
loss: 1.48578	cur:[300]\[300]
save model at ./check_point_fusion/epoch80.pth
epoch:80	avg_epoch_loss:1.48667
epoch:80	bleu:0.05069
*************** epoch:81 ***************
loss: 1.21745	cur:[100]\[1900]
loss: 1.24054	cur:[200]\[1900]
loss: 1.24970	cur:[300]\[1900]
loss: 1.25093	cur:[400]\[1900]
loss: 1.23021	cur:[500]\[1900]
loss: 1.21781	cur:[600]\[1900]
loss: 1.20201	cur:[700]\[1900]
loss: 1.19123	cur:[800]\[1900]
loss: 1.19186	cur:[900]\[1900]
loss: 1.25471	cur:[1000]\[1900]
loss: 1.22205	cur:[1100]\[1900]
loss: 1.18783	cur:[1200]\[1900]
loss: 1.18563	cur:[1300]\[1900]
loss: 1.22026	cur:[1400]\[1900]
loss: 1.20972	cur:[1500]\[1900]
loss: 1.22344	cur:[1600]\[1900]
loss: 1.17450	cur:[1700]\[1900]
loss: 1.20886	cur:[1800]\[1900]
loss: 1.22355	cur:[1900]\[1900]
epoch:81	avg_epoch_loss:1.21591
--------------- Evaluation ---------------
loss: 1.51569	cur:[100]\[300]
loss: 1.51793	cur:[200]\[300]
loss: 1.51350	cur:[300]\[300]
epoch:81	avg_epoch_loss:1.51571
epoch:81	bleu:0.04256
*************** epoch:82 ***************
loss: 1.17875	cur:[100]\[1900]
loss: 1.27511	cur:[200]\[1900]
loss: 1.22027	cur:[300]\[1900]
loss: 1.19210	cur:[400]\[1900]
loss: 1.21440	cur:[500]\[1900]
loss: 1.12991	cur:[600]\[1900]
loss: 1.20073	cur:[700]\[1900]
loss: 1.16162	cur:[800]\[1900]
loss: 1.21364	cur:[900]\[1900]
loss: 1.28727	cur:[1000]\[1900]
loss: 1.18671	cur:[1100]\[1900]
loss: 1.19879	cur:[1200]\[1900]
loss: 1.20251	cur:[1300]\[1900]
loss: 1.16744	cur:[1400]\[1900]
loss: 1.19952	cur:[1500]\[1900]
loss: 1.24057	cur:[1600]\[1900]
loss: 1.21254	cur:[1700]\[1900]
loss: 1.20217	cur:[1800]\[1900]
loss: 1.27385	cur:[1900]\[1900]
epoch:82	avg_epoch_loss:1.20831
--------------- Evaluation ---------------
loss: 1.46006	cur:[100]\[300]
loss: 1.53717	cur:[200]\[300]
loss: 1.58446	cur:[300]\[300]
epoch:82	avg_epoch_loss:1.52723
epoch:82	bleu:0.03771
*************** epoch:83 ***************
loss: 1.20573	cur:[100]\[1900]
loss: 1.29747	cur:[200]\[1900]
loss: 1.27514	cur:[300]\[1900]
loss: 1.23440	cur:[400]\[1900]
loss: 1.13332	cur:[500]\[1900]
loss: 1.21990	cur:[600]\[1900]
loss: 1.20596	cur:[700]\[1900]
loss: 1.15646	cur:[800]\[1900]
loss: 1.19371	cur:[900]\[1900]
loss: 1.20960	cur:[1000]\[1900]
loss: 1.22165	cur:[1100]\[1900]
loss: 1.21057	cur:[1200]\[1900]
loss: 1.23781	cur:[1300]\[1900]
loss: 1.20654	cur:[1400]\[1900]
loss: 1.21489	cur:[1500]\[1900]
loss: 1.18048	cur:[1600]\[1900]
loss: 1.19258	cur:[1700]\[1900]
loss: 1.21512	cur:[1800]\[1900]
loss: 1.20208	cur:[1900]\[1900]
epoch:83	avg_epoch_loss:1.21123
--------------- Evaluation ---------------
loss: 1.43614	cur:[100]\[300]
loss: 1.61551	cur:[200]\[300]
loss: 1.54926	cur:[300]\[300]
epoch:83	avg_epoch_loss:1.53363
epoch:83	bleu:0.03602
*************** epoch:84 ***************
loss: 1.23890	cur:[100]\[1900]
loss: 1.21241	cur:[200]\[1900]
loss: 1.17173	cur:[300]\[1900]
loss: 1.19060	cur:[400]\[1900]
loss: 1.18108	cur:[500]\[1900]
loss: 1.14729	cur:[600]\[1900]
loss: 1.21795	cur:[700]\[1900]
loss: 1.26226	cur:[800]\[1900]
loss: 1.17967	cur:[900]\[1900]
loss: 1.23487	cur:[1000]\[1900]
loss: 1.22368	cur:[1100]\[1900]
loss: 1.12301	cur:[1200]\[1900]
loss: 1.24955	cur:[1300]\[1900]
loss: 1.21103	cur:[1400]\[1900]
loss: 1.25088	cur:[1500]\[1900]
loss: 1.18639	cur:[1600]\[1900]
loss: 1.17298	cur:[1700]\[1900]
loss: 1.19786	cur:[1800]\[1900]
loss: 1.18632	cur:[1900]\[1900]
epoch:84	avg_epoch_loss:1.20202
--------------- Evaluation ---------------
loss: 1.42173	cur:[100]\[300]
loss: 1.45847	cur:[200]\[300]
loss: 1.48965	cur:[300]\[300]
epoch:84	avg_epoch_loss:1.45662
epoch:84	bleu:0.03537
*************** epoch:85 ***************
loss: 1.13879	cur:[100]\[1900]
loss: 1.14501	cur:[200]\[1900]
loss: 1.17322	cur:[300]\[1900]
loss: 1.18098	cur:[400]\[1900]
loss: 1.18876	cur:[500]\[1900]
loss: 1.18572	cur:[600]\[1900]
loss: 1.19203	cur:[700]\[1900]
loss: 1.23259	cur:[800]\[1900]
loss: 1.20546	cur:[900]\[1900]
loss: 1.19341	cur:[1000]\[1900]
loss: 1.18686	cur:[1100]\[1900]
loss: 1.21479	cur:[1200]\[1900]
loss: 1.22783	cur:[1300]\[1900]
loss: 1.19035	cur:[1400]\[1900]
loss: 1.20392	cur:[1500]\[1900]
loss: 1.26388	cur:[1600]\[1900]
loss: 1.18408	cur:[1700]\[1900]
loss: 1.24692	cur:[1800]\[1900]
loss: 1.22379	cur:[1900]\[1900]
epoch:85	avg_epoch_loss:1.19886
--------------- Evaluation ---------------
loss: 1.48167	cur:[100]\[300]
loss: 1.48765	cur:[200]\[300]
loss: 1.60546	cur:[300]\[300]
epoch:85	avg_epoch_loss:1.52492
epoch:85	bleu:0.03597
*************** epoch:86 ***************
loss: 1.21620	cur:[100]\[1900]
loss: 1.16338	cur:[200]\[1900]
loss: 1.22211	cur:[300]\[1900]
loss: 1.17313	cur:[400]\[1900]
loss: 1.21664	cur:[500]\[1900]
loss: 1.21355	cur:[600]\[1900]
loss: 1.23404	cur:[700]\[1900]
loss: 1.25663	cur:[800]\[1900]
loss: 1.21817	cur:[900]\[1900]
loss: 1.23952	cur:[1000]\[1900]
loss: 1.30867	cur:[1100]\[1900]
loss: 1.12020	cur:[1200]\[1900]
loss: 1.16951	cur:[1300]\[1900]
loss: 1.25054	cur:[1400]\[1900]
loss: 1.23330	cur:[1500]\[1900]
loss: 1.14856	cur:[1600]\[1900]
loss: 1.15989	cur:[1700]\[1900]
loss: 1.23625	cur:[1800]\[1900]
loss: 1.22295	cur:[1900]\[1900]
epoch:86	avg_epoch_loss:1.21070
--------------- Evaluation ---------------
loss: 1.49169	cur:[100]\[300]
loss: 1.57562	cur:[200]\[300]
loss: 1.55493	cur:[300]\[300]
epoch:86	avg_epoch_loss:1.54075
epoch:86	bleu:0.03398
*************** epoch:87 ***************
loss: 1.16537	cur:[100]\[1900]
loss: 1.22245	cur:[200]\[1900]
loss: 1.21486	cur:[300]\[1900]
loss: 1.16709	cur:[400]\[1900]
loss: 1.21283	cur:[500]\[1900]
loss: 1.20148	cur:[600]\[1900]
loss: 1.24449	cur:[700]\[1900]
loss: 1.19853	cur:[800]\[1900]
loss: 1.21084	cur:[900]\[1900]
loss: 1.17111	cur:[1000]\[1900]
loss: 1.14638	cur:[1100]\[1900]
loss: 1.23045	cur:[1200]\[1900]
loss: 1.19925	cur:[1300]\[1900]
loss: 1.27950	cur:[1400]\[1900]
loss: 1.24913	cur:[1500]\[1900]
loss: 1.20033	cur:[1600]\[1900]
loss: 1.23342	cur:[1700]\[1900]
loss: 1.21042	cur:[1800]\[1900]
loss: 1.20633	cur:[1900]\[1900]
epoch:87	avg_epoch_loss:1.20865
--------------- Evaluation ---------------
loss: 1.42543	cur:[100]\[300]
loss: 1.51126	cur:[200]\[300]
loss: 1.42652	cur:[300]\[300]
epoch:87	avg_epoch_loss:1.45440
epoch:87	bleu:0.04697
*************** epoch:88 ***************
loss: 1.15773	cur:[100]\[1900]
loss: 1.23963	cur:[200]\[1900]
loss: 1.21224	cur:[300]\[1900]
loss: 1.18690	cur:[400]\[1900]
loss: 1.24761	cur:[500]\[1900]
loss: 1.18552	cur:[600]\[1900]
loss: 1.30653	cur:[700]\[1900]
loss: 1.13219	cur:[800]\[1900]
loss: 1.20975	cur:[900]\[1900]
loss: 1.21648	cur:[1000]\[1900]
loss: 1.20033	cur:[1100]\[1900]
loss: 1.21243	cur:[1200]\[1900]
loss: 1.19804	cur:[1300]\[1900]
loss: 1.16605	cur:[1400]\[1900]
loss: 1.23621	cur:[1500]\[1900]
loss: 1.18322	cur:[1600]\[1900]
loss: 1.17791	cur:[1700]\[1900]
loss: 1.19206	cur:[1800]\[1900]
loss: 1.21098	cur:[1900]\[1900]
epoch:88	avg_epoch_loss:1.20378
--------------- Evaluation ---------------
loss: 1.49203	cur:[100]\[300]
loss: 1.44480	cur:[200]\[300]
loss: 1.55886	cur:[300]\[300]
epoch:88	avg_epoch_loss:1.49856
epoch:88	bleu:0.03880
*************** epoch:89 ***************
loss: 1.27443	cur:[100]\[1900]
loss: 1.17529	cur:[200]\[1900]
loss: 1.21980	cur:[300]\[1900]
loss: 1.24231	cur:[400]\[1900]
loss: 1.20563	cur:[500]\[1900]
loss: 1.21913	cur:[600]\[1900]
loss: 1.17764	cur:[700]\[1900]
loss: 1.21823	cur:[800]\[1900]
loss: 1.12302	cur:[900]\[1900]
loss: 1.22541	cur:[1000]\[1900]
loss: 1.22629	cur:[1100]\[1900]
loss: 1.24934	cur:[1200]\[1900]
loss: 1.23091	cur:[1300]\[1900]
loss: 1.22416	cur:[1400]\[1900]
loss: 1.15787	cur:[1500]\[1900]
loss: 1.17692	cur:[1600]\[1900]
loss: 1.15230	cur:[1700]\[1900]
loss: 1.22069	cur:[1800]\[1900]
loss: 1.25875	cur:[1900]\[1900]
epoch:89	avg_epoch_loss:1.20937
--------------- Evaluation ---------------
loss: 1.51511	cur:[100]\[300]
loss: 1.52830	cur:[200]\[300]
loss: 1.53241	cur:[300]\[300]
epoch:89	avg_epoch_loss:1.52528
epoch:89	bleu:0.03927
*************** epoch:90 ***************
loss: 1.25886	cur:[100]\[1900]
loss: 1.19698	cur:[200]\[1900]
loss: 1.21176	cur:[300]\[1900]
loss: 1.18497	cur:[400]\[1900]
loss: 1.18674	cur:[500]\[1900]
loss: 1.28282	cur:[600]\[1900]
loss: 1.19535	cur:[700]\[1900]
loss: 1.22377	cur:[800]\[1900]
loss: 1.20212	cur:[900]\[1900]
loss: 1.19024	cur:[1000]\[1900]
loss: 1.17834	cur:[1100]\[1900]
loss: 1.15870	cur:[1200]\[1900]
loss: 1.16093	cur:[1300]\[1900]
loss: 1.19255	cur:[1400]\[1900]
loss: 1.23071	cur:[1500]\[1900]
loss: 1.22561	cur:[1600]\[1900]
loss: 1.27996	cur:[1700]\[1900]
loss: 1.24686	cur:[1800]\[1900]
loss: 1.22982	cur:[1900]\[1900]
epoch:90	avg_epoch_loss:1.21248
--------------- Evaluation ---------------
loss: 1.55051	cur:[100]\[300]
loss: 1.48374	cur:[200]\[300]
loss: 1.40694	cur:[300]\[300]
save model at ./check_point_fusion/epoch90.pth
epoch:90	avg_epoch_loss:1.48040
epoch:90	bleu:0.04343
*************** epoch:91 ***************
loss: 1.21763	cur:[100]\[1900]
loss: 1.18607	cur:[200]\[1900]
